---
title: "Sociology Abstracts"
format: html
editor: visual
toc: TRUE
code-tools: TRUE
code-fold: TRUE
embed-resources: TRUE
---

## Setup

Load data from proquest on 2020-2024-02-01 sociology department dissertations in English. Clean and tokenise as 2-skip-1 ngrams

```{r setup, include=FALSE}
set.seed(seed=NULL)
library("tidyverse", quietly=TRUE, warn.conflicts=FALSE)
library("magrittr",include.only="%<>%", quietly=TRUE, warn.conflicts=FALSE)
library("tidytext", quietly=TRUE, warn.conflicts=FALSE)
requireNamespace("textstem")
requireNamespace("plotly")
requireNamespace("DT")
requireNamespace("ggwordcloud")
requireNamespace("reshape2")
requireNamespace("topicmodels")
```

```{r global-processiong-options}
DOSTEM <- FALSE
```

```{r retrieve-expertcodings}
if (!file.exists("data/coded_abstracts.xlsx") || 
    !file.exists("data/ngram_seeds.xlsx")   || TRUE
    ) {
requireNamespace("googledrive")
folderid <- googledrive::as_id("https://drive.google.com/drive/folders/1eFat4mCoRZX22gPLTj9w5Bn63U3q71yo")

filels <- googledrive::drive_ls(folderid)

googledrive::drive_download(
  file = filels %>% filter(name=="abstracts_hand_coding") %>% pull(`id`),
  path = "data/coded_abstracts",
  type= "xlsx",
  overwrite=TRUE)

googledrive::drive_download(
  file = filels %>% filter(name=="socabs_topic_seeds") %>% pull(`id`),
  path = "data/ngram_seeds",
  type= "xlsx",
  overwrite=TRUE)

googledrive::drive_download(
  file = filels %>% filter(name=="stems_and_stops") %>% pull(`id`),
  path = "data/stems_and_stops",
  type= "xlsx",
  overwrite=TRUE)

 rm(folderid, filels)
}
```

```{r, load-abs}
abs1.df <- readxl::read_excel("data/ProQuestDocuments-2024-02-01.xls")
abs2.df <- readxl::read_excel("data/ProQuestDocuments-2024-02-01.2.xls")
diss_meta.df <- bind_rows(abs1.df, abs2.df)
rm("abs1.df","abs2.df")

# NOTE:
# Columns in source overal with each other -- only selected columns used
#  - all duplicates: classification, subjectClassifications, classiifcationCOdes, majorClassificationsCodes
# - subjectTerms appears to be an automated coarse recoding of the classification
# - appears author assigned, duplicate columns: identifierKeywords, subjects
#   apparently post-processed to add "GenderWatch" and "y" tags 

diss_cleaned.df <- diss_meta.df %>% 
  select(isbn, Authors, classification, subjectTerms, pubdate,
         Abstract, Title, identifierKeywords) %>%
  mutate(classification = replace_na(classification,""),
         subjectTerms = replace_na(subjectTerms,""),
         identifierKeywords = replace_na(identifierKeywords,""),
         pubyear_clean = year(as_date(pubdate,format="%Y"))
         ) %>%
  rowwise() %>%
  mutate(classification_clean = 
          str_split_1(classification ,pattern = ",") %>% 
           str_squish() %>%
          str_replace("^[0-9]+ ","") %>%
          str_to_lower() %>% unique() %>% list() ,
         subject_terms_clean = str_split_1(subjectTerms,pattern = ",") %>%
           str_squish() %>% str_to_lower() %>% unique() %>%
           list(), 
          au_identifier_terms_clean =
           str_split_1(identifierKeywords,pattern = ",") %>%
           str_squish() %>% str_to_lower() %>% unique() %>%
           list()
  ) %>% 
    ungroup() %>%
    select(isbn, Authors, classification_clean, subject_terms_clean, pubyear_clean,
           Abstract,Title,identifierKeywords, au_identifier_terms_clean)

diss_cleaned.df %>%
  count(pubyear_clean) %>%
  rename(year=pubyear_clean)-> dy.df
dy.ls <- dy.df %>% pull(n)
names(dy.ls) <- dy.df %>% pull(year)

rm(diss_meta.df)
```

```{r tokenize}
# subset of the snowball stopwords

minimal_stopwords <-
  c("a",  "am", "an", "and", "any", "are", "aren't", "as", "at", "be","but", "by",  "did", "didn't", 
"do", "does", "doesn't", "doing", "don't", "down", "during", 
"each", "for", "from", "further", "had", "hadn't", "has", 
"hasn't", "have", "haven't", "having", "how", "i", "i'd", "i'll", "i'm", "i've", "if", "in", "into", "is", "isn't", "it", "it's", "its", "itself", "let's", "me", "my", "myself",  "of", "on",  "or", "other",  "so", "than", "that", "that's", "the","their", "they", "them", "then", "there", "there's", "these", "this", "how", "to", "too",  "was", "wasn't", "when", "when's", "where", "where's", "which", "while", "will", "with", "won't")

combined_tidy.df <-  diss_cleaned.df %>%
  select(isbn,Abstract,Title,identifierKeywords) %>%
  unite(col="Clean_combined", sep=" ", remove=TRUE, na.rm=TRUE,
        Abstract,Title,identifierKeywords) %>%
  mutate(Clean_combined =
           str_replace_all(Clean_combined, "U\\.S\\.","USA")) %>%
  mutate(Clean_combined =
           str_replace_all(Clean_combined, "[-\\)\\(\\&\"/]"," ")) %>%
  mutate(Clean_combined =
           str_replace_all(Clean_combined, "[^a-zA-Z \\.']", ""))  %>% 
  mutate(Clean_combined =
           str_squish(Clean_combined)) %>% 
  unnest_tokens(ngram, "Clean_combined", token = "skip_ngrams", n = 2,k=1,
                stopwords=minimal_stopwords)  %>%
  mutate(ngram = str_squish(ngram)) %>% 
  filter(!str_detect(ngram,"\\.")) %>%
  filter(ngram!="")

if (DOSTEM) {
  combined_tidy.df %<>%
     mutate(ngram = textstem::stem_strings(ngram, language="english"))
}

combined_tf_idf.df <- 
  combined_tidy.df %>% 
  count(ngram,isbn) %>%
    bind_tf_idf(ngram, isbn, n)
```

```{r post-stop-and-stem}
stop_ngrams <- c( 
"associated with", 
"interviews with",  
"these findings", 
"with their", 
"my dissertation",
"first chapter",
"second chapter", 
"empirical chapter", 
"my research", 
"depth with", 
"my findings",
"dissertation three", 
"et al",
"third chapter", 
"three studies",
"their with", 
"with social",  
"with other", 
"interactions with",
"taken together", 
"empirical chapters",
"my study", 
"they also", 
"women who",
"who were", 
"findings study", 
"people who", 
"relationships with", 
"these results", 
"their their", 
"these studies", 
"they were", 
"attitudes toward", 
"since s", 
"these suggest", 
"three papers", 
"women with", 
"along with", 
"these two",
"with more", 
"students their",  
"their identities", 
"their health", 
"their parents", 
"were likely", 
"with these", 
"chapter two", 
"chapter uses",
"their counterparts",
"with lower",  
"structured with", 
"students with", 
"study explores", 
"these three", 
"together these", 
"health health", 
"students who",
"th century", 
"their identity",
"third study",
"also find",
"chapter three", 
"across three", 
"all three", 
"chapter examine",
"one another", 
"previous research", 
"second paper", 
"were with", 
"first paper", 
"final chapter", 
"with focus", 
"study also", 
"with mental", 
"with parents", 
"with women", 
"women their", 
"social social", 
"within context", 
"individuals with", 
"other hand", 
"other social",  
"following questions", 
"three questions", 
"individuals who", 
"third paper", 
"three distinct",
"while also", 
"with high", 
"address these", 
"children with", 
"conducted with",
"dissertation social", 
"existing literature", 
"health among", 
"results study", 
"their communities", 
"there little", 
"what call", 
"who their", 
"with family", 
"among women", 
"different social", 
"experiences their", 
"findings dissertation", 
"research also", 
"study examine", 
"their political", 
"these factors", 
"they navigate", 
"with black", 
"with data", 
"with levels", 
"with one", 
"women more",
"engagement with",
"finally chapter", 
"level factors", 
"little known", 
"men who", 
"more their", 
"with racial", 
"adults with",
"data with",
"dissertation two", 
"engage with",
"more specifically",
"study social", 
"with different", 
"with health", 
"chapter four", 
"cope with", 
"different types",
"existing research", 
"one most", 
"parents their",
"social dissertation", 
"their they", 
"these changes", 
"these processes", 
"were conducted", 
"who with", 
"with illness", 
"with people", 
"among older", 
"contributes understanding", 
"current study", 
"interact with", 
"late s",
"overall dissertation", 
"people their", 
"prior research", 
"their family", "their status", "them their", "they with", "vary across", 
"what they", "with who", "across different", 
"across united", "chapter analyzes", "data study", 
"however these", "level data", 
"research question", 
"study uses", "support their", 
"these experiences", "these were", "within their", "work with", 
 "based their",  "chapter investigates", 
 "different groups", "early s", 
  "first examine", 
"individuals their",
"methods approach", 
"months ethnographic", 
"months fieldwork", 
"their experience", 
"their relationships", 
"third empirical", 
"with political", "with students", "years ethnographic", "across all", 
"across groups", "among adults", "children their", 
"different ways", "dissertation with", "impact their", "research social", 
"shape their", "there research", "these data", "using methods", 
"were associated",  "association with", 
 "even when", "examine social", "few studies", "findings also", 
"first two", 
"relationships their", "sense their", 
"their these", "these can", "they can", 
"were less", 
"with greater", "with increased", "chapter dissertation", 
"conclude with", 
"dissertation research", "dissertation use", "questions what", 
"three different", "three dissertation", 
"two different", "with non",
"within social", "women were",  
 "implications research", "mid s", "positively with", 
 "st century", "study chapter", "study three", 
 "these analyses",  "were used", "what extent", "while research", 
"with implications", "with low", "with self", 
"work their",  "other words",  "research with", 
 "their role", "these also", 
"these dissertation", "two chapters", 
"were significantly", "when their",  "with men", 
"with new", "with respect",  
"due their",  "first dissertation",  
"however there", 
"most important", "my first", "my suggest", "social well", "study two", 
"themselves their", "these groups", "these strategies", "three key", 
 "what can",  
"can help",  "consists three", 
 "dissertation also", "experience their", 
"explore these",
"we find", "were also",  "with discussion", 
"also their",  "combined with", "compared their", 
"contact with", "data depth", "data three", "dissertation study", 
"finally study",  
"findings research",
 "methods used", "more more", "more with", 
"other forms", "overall findings", 
"overall study",  "research focused", 
"results these", "school with",  "study with",  
"these chapters",  "three years", 
 "using case", "work conflict",
 "chapter whether", 
 "correlated with", "data dissertation", 
"data were", "despite their",  "however research", 
"men their", "my results",  "other factors", 
 "present study", "research these", "research what", 
"states with", "structural cultural", "study adult", "their everyday", 
 "these provide", "these spaces", 
"three research", "use their", 
"also more", "also show",  "concludes with", 
"examines social",    
"level characteristics", "much more", "my also", 
"my show", "my work", 
"outcomes with", "taken these", 
"their first", "their interactions", "them with", 
 "these differences", "these issues", "these outcomes", 
"these research", "they did", "they face",
"genderwatch"
)

stop_post_tokens<- c("with", "", "social", "these", "dissertation", "study", 
"research", "more", "they", "chapter",  
 "also", "three",  "who", 
"first",  "were",  "findings", 
"analysis", "use", "while", "within", "two", "my", "other", 
"using", "across", "among", "can", "level", 
 "based",  
 "find", "well", 
"what",  "time", "second", "when",  "studies",  "one",  
"may", "care", 
"however",
"non", 
"them", 
"all",  "there", "used", 
"third", "including",  "s", 
 "four",  "upon", 
 "n", 
"y",
"co", "xa", "five", "re", "al", "et", 
"et al")

stop_post.df <- tibble(ngram=c(stop_post_tokens, stop_ngrams))
stop_post_minimal.df <- tibble(ngram=c("with"))

combined_tf_idf_stemmed.df <- combined_tf_idf.df
rm(stop_post_tokens,stop_ngrams)
```

# Distributions of topics (unclustered)

## Dissertations by Controlled Classifications

```{r controlled-meta-clean}
diss_cleaned.df %>% 
  rename(term=classification_clean) %>%
  select(isbn, term, pubyear_clean) %>% 
  unnest(cols=c(term)) -> diss_class_tidy.df

ndis <- sum(dy.ls)
lower_q <- .05
upper_q <- .8

{diss_class_tidy.df %>% 
  count(term, sort=TRUE) %>%
  mutate(p = n/sum(dy.ls)) %>%
  filter(p>= lower_q ,
         p <= upper_q ) %>%
  ggplot(aes(x=fct_reorder(term,p),y=p))+
  geom_col() +
  coord_flip() +
  labs(x="%age of dissertation assigned to controlled classifications")} %>% plotly::ggplotly()

{diss_class_tidy.df %>% 
  filter(pubyear_clean < 2024) %>%
  count(term, pubyear_clean, sort=TRUE) %>%
  rowwise() %>%
  mutate(p=n/ndis,
         p_year=n/dy.ls[[as.character(pubyear_clean)]]) %>%
  filter(p_year >=  lower_q ,
         p_year <=  upper_q ) %>%
  ggplot(aes(x=fct_reorder(term,p),y=p_year))+
  geom_col() +
  coord_flip() +
  labs(x="%age dissertations in assigned  classification over time") +
  facet_wrap(vars(pubyear_clean))} %>% plotly::ggplotly() 

rm(diss_class_tidy.df, lower_q, upper_q)
```

## Dissertations by Author-Assigned Topics

```{r au-topics}
stop_topics.df <- tibble(term=c("y","genderwatch"))

diss_cleaned.df %>% 
  rename(term=au_identifier_terms_clean) %>%
  select(isbn, term, pubyear_clean) %>% 
  unnest(cols=c(term))  %>%
  anti_join(stop_topics.df,by="term")-> diss_au_id_tidy.df

lower_q <- .025
upper_q <- 1

{diss_au_id_tidy.df %>% 
  count(term, sort=TRUE) %>%
  mutate(p = n/sum(dy.ls)) %>%
  filter(p>= lower_q ,
         p <= upper_q ) %>%
  ggplot(aes(x=fct_reorder(term,p),y=p))+
  geom_col() +
  coord_flip() +
  labs(x="%age of dissertation by author-assigned keywords")} %>% plotly::ggplotly()

rm(diss_au_id_tidy.df, stop_topics.df)
```

## Dissertations by Terminology

```{r terms-dis-frequencies}
unc_doc_freq.df <-
  combined_tf_idf_stemmed.df %>% 
  count(ngram) %>%
  mutate(p=n/sum(dy.ls))

lower_q <- .25 
upper_q <- .75

unc_doc_freq.df  %>% 
      slice_max(order_by=n, n=200) %>%
      rename(freq=n,word=ngram) %>%
      ggwordcloud::ggwordcloud2(size=.8) +
      labs(title="terms appearing in most dissertations - excluding stopwords") 

{unc_doc_freq.df %>% 
  mutate(p = n/sum(dy.ls),
         ngram=fct_reorder(ngram,p)) %>%
  filter(p>= lower_q,
         p<= upper_q) %>%
  ggplot(aes(x=ngram,y=p))+
  geom_col() +
  coord_flip() +
  labs(x="terms appearing in [25%-75%] of dissertations (excluding stopwords, min 1%)")} %>% plotly::ggplotly()

unc_doc_freq.df  %>% 
    slice_max(order_by=n, n=1000) %>%
    DT::datatable (
      data = .,
      extensions = 'Buttons',
      options = list(dom = 'Bfrltip',
                     buttons = c('csv')),
      caption ="ngrams appearing in most dissertations"
    )

rm(unc_doc_freq.df, lower_q, upper_q)
```

## Distribution of Terminology Across Corpus
```{r uncontrolled-terms-frequencies}
unc_ngram_freq.df <- 
  combined_tf_idf_stemmed.df %>% 
  group_by(ngram) %>%
  summarise(n=sum(n), .groups="drop") %>%
  arrange(n)

unc_ngram_freq.df  %>% 
      slice_max(order_by = n,n=200) %>%
      rename(freq=n,word=ngram) %>%
      ggwordcloud::ggwordcloud2(size=.8) +
      labs(title="most popular uncontrolled terms - excluding stop words") 
    
unc_ngram_freq.df  %>% 
    slice_max(order_by = n,n=1000) %>%
    DT::datatable (
      data = .,
      extensions = 'Buttons',
      options = list(dom = 'Bfrltip',
                     buttons = c('csv')),
      caption ="Most frequent 1 and 2 word terms in uncontrolled description"
    )
rm(unc_ngram_freq.df)
```

## Most Distinctive Terms in Each Dissertation

```{r tdf-explore}
nterms <- 5
ndiss <- 100

topterms.df <-
  combined_tf_idf_stemmed.df %>% 
  group_by(isbn) %>%
  slice_max(order_by=tf_idf, n=nterms) 

topdis.df <-
  topterms.df %>%
  group_by(isbn) %>%
  summarize(mean_tf_idf = mean(tf_idf)) %>%
  ungroup() %>%
  slice_max(order_by=mean_tf_idf, n= ndiss)
  
distinctive_diss.df <-
  left_join(topdis.df %>% select(isbn),
            topterms.df %>% select(isbn, ngram),
            by = "isbn") %>% 
  group_by(isbn) %>%
  summarize(distinct_terms = paste(ngram, sep =" ", collapse=", ")) %>% 
  ungroup() %>%
  left_join(diss_cleaned.df %>% select(Title,isbn), by="isbn")

distinctive_diss.df %>%
  relocate(Title, distinct_terms) %>%
  select(-isbn) %>%
  DT::datatable (
      data = .,
      extensions = 'Buttons',
      options = list(dom = 'Bfrltip',
                     buttons = c('csv')),
      caption ="Most distinctive terms in dissertations with distinctive terms"
    )
 
rm(nterms,ndiss, topterms.df, topdis.df, distinctive_diss.df)
```

# Topic Coding

```{r create-dictionary}
coded_abs.df <- readxl::read_excel("data/coded_abstracts.xlsx")

#TODO: stem seeds
#TODO: flag seeds missing in corpus

seeds_from_codes.df <- coded_abs.df %>% 
  select(contains(':')) %>% 
  pivot_longer(cols=everything(), names_to="topic", values_to = "ngram") %>% 
  filter(!is.na(ngram) & ngram != 'x') %>%
  separate_longer_delim(ngram,';') %>%
  mutate(ngram=str_squish(ngram)) %>%
  filter(ngram!='')

seeds_from_ngrams.df <- 
  suppressMessages( readxl::read_excel("data/ngram_seeds.xlsx") )%>%
  rename(topic=Topic,ngram=Ngram) %>% select(topic,ngram)

seeds_from_ngrams.df %<>% 
  separate_longer_delim(topic,";") %>% 
  filter(!is.na(`topic`) & str_detect(`topic`,":")) %>%
  mutate(topic = str_squish(`topic`))  %>%
  filter(topic!="")

seeds.df <- bind_rows(seeds_from_codes.df, seeds_from_ngrams.df) %>% 
  mutate(ngram = str_to_lower(ngram) %>% 
           str_replace_all(pattern="-"," ") %>%
           str_squish()
           ) %>%
  distinct()

rm(seeds_from_codes.df, seeds_from_ngrams.df)
```

## Hand Coded Abstracts

```{r hand-coded}
hand_codes_tmp.df <-
  coded_abs.df %>% select(contains(':'),EpiStyle_type) %>% 
  filter( if_any(everything(), ~ !is.na(.x)) )


epi_tot.df <- hand_codes_tmp.df %>%
  select(EpiStyle_type) %>% 
  rename(category=EpiStyle_type) %>%
  count(category) %>%
  mutate(dimension="epi")

hand_codes_tmp.df %>% select(-EpiStyle_type) %>%
    mutate(across(everything(), ~ case_when(is.na(.x) ~ FALSE,  .default = TRUE))) %>%
    pivot_longer(cols=everything()) %>%
    count(name,value) %>%
    filter(value) %>%
    select(-value) %>%
    separate_wider_delim(name,names=c("dimension","category"),delim=":") %>%
    bind_rows(epi_tot.df)  -> hand_code_tot.df

{hand_code_tot.df %>%
    mutate(dimension = as.factor(dimension), category=as.factor(category)) %>%
    ggplot(aes(y=n,x=dimension,  fill=category,label=category)) +
    geom_col(position=position_dodge2()) +
    geom_text(position=position_dodge2(width=1), vjust=1) +
    theme(legend.position="none") + 
    facet_wrap(vars(dimension), scales="free_x")} %>%
  plotly::ggplotly()
  
  #rm(epi_tot.df, hand_codes_tmp.df, hand_code_tot.df) 
```

\

## Exact Match to Coded Topics Terms

```{r exact-matches}
exact_matches.df <- 
  left_join(seeds.df,
            combined_tf_idf_stemmed.df %>% distinct(),
            relationship="many-to-many",
            by = "ngram"
            )

nmatched_dis <- exact_matches.df %>% pull(isbn) %>% unique() %>% length

exact_matches.df %>% 
  select(-ngram) %>% 
  distinct() %>% 
  count(topic) %>%
  ungroup() %>%
  
  separate_wider_delim(topic,names=c("dimension","category"),delim=":") -> topic_sum.df 

topic_sum.df %<>% 
  group_by(dimension) %>%
  mutate(p=n/sum(dy.ls)) %>%
  ungroup()

{topic_sum.df%>%
  ggplot(aes(y=p,x=dimension,fill=category,label=category)) +
  geom_col(position=position_dodge2(width=1)) +
  geom_text(position=position_dodge2(width=1), vjust=1) +
  theme(legend.position="none") + 
  facet_wrap(vars(dimension), scales="free_x")} %>% plotly::ggplotly()

topic_sum.df %>% 
  group_by(dimension) %>%
  gt::gt() %>%
  gt::fmt_percent(columns="p")

rm(topic_sum.df)
```

## Topic Models

```{r keyword-assisted-topics}

#requireNamespace("quanteda")
requireNamespace("keyATM")

combined_tf_idf_stemmed.df %>% 
  group_by(ngram) %>%
  summarize(n=n()) %>%
  mutate(p=n/sum(dy.ls)) -> ngram_diss_ct.df

lower_q <- .025 
upper_q <- .975

core_ngrams.df  <-
  ngram_diss_ct.df %>%
  filter( p >= lower_q, p<= upper_q) %>%
  select("ngram") %>%
  bind_rows(seeds.df %>% select(ngram))  %>%
  distinct() %>%
  left_join(combined_tf_idf_stemmed.df %>% select(ngram,isbn,n)) %>%
  na.omit() %>%
  distinct()

core_dfm <- 
  core_ngrams.df %>%
  rename(term=ngram, document=isbn, value=n) %>% 
  cast_dfm(document=document,term=term,value=value)

keyATM_docs <- keyATM::keyATM_read(texts = core_dfm)
summary(keyATM_docs)

  
seeds.df %>% 
  filter(!stringr::str_starts(topic,"epi:")) %>%
  group_by(`topic`) %>% 
  summarise(ngram_list=list(ngram)) %>% 
  pmap( function(topic,ngram_list) { x<-list(); x[[topic]] <- ngram_list; x} ) %>% 
  list_flatten() -> seeds.ls

key_viz <- keyATM::visualize_keywords(docs = keyATM_docs, keywords = seeds.ls)
key_viz

suppressMessages(lda_key <- keyATM::keyATM(
  docs              = keyATM_docs,    # text input
  no_keyword_topics = 5,              # number of topics without keywords
  keywords          = seeds.ls,       # keywords
  model             = "base",         # select the model
))

lda_key %>% keyATM::plot_modelfit()
```

```{r key-atm-results}

lda_key %>% keyATM::top_words()  %>%
  gt::gt() %>% 
  gt::tab_header("Top words for each topic")
  
lda_key %>% keyATM::top_docs(n=5) %>% 
  pivot_longer(cols=everything()) %>%
  rowwise() %>%
  mutate(title=diss_cleaned.df[[value,"Title"]]) %>%
  ungroup() %>% 
  select(-value) %>%
  group_by(name) %>%
  gt::gt() %>%
  gt::tab_header("Top dissertations in each topic")


{lda_key %>% keyATM::plot_topicprop(
  n = 5,
  show_topic = NULL,
  show_topwords = TRUE,
  label_topic = NULL,
  order = "proportion")}[["figure"]] %>%
  plotly::ggplotly()


```

# Author Characteristics

```{r gender-coding}
diss_cleaned_plus_gender.df <- diss_cleaned.df %>% 
  mutate(given=  str_split_i(Authors,pattern=",",2)
         %>% str_squish() %>% 
           str_split_i(pattern="[:space:]",1)) 

diss_cleaned_plus_gender.df %<>% opengender::add_gender_predictions()
```
