---
title: "Sociology Abstracts"
format: html
editor: visual
toc: TRUE
code-tools: TRUE
code-fold: TRUE
embed-resources: TRUE
---

## Setup

Load data from proquest on 2020-2024-02-01 sociology department dissertations in English. Clean and tokenise as 2-skip-1 ngrams

```{r setup, include=FALSE}
set.seed(seed=NULL)
library("tidyverse", quietly=TRUE, warn.conflicts=FALSE)
library("magrittr",include.only="%<>%", quietly=TRUE, warn.conflicts=FALSE)
library("tidytext", quietly=TRUE, warn.conflicts=FALSE)
requireNamespace("textstem")
requireNamespace("plotly")
requireNamespace("DT")
requireNamespace("ggwordcloud")
requireNamespace("reshape2")
requireNamespace("topicmodels")
```

```{r, load-abs}
abs1.df <- readxl::read_excel("data/ProQuestDocuments-2024-02-01.xls")
abs2.df <- readxl::read_excel("data/ProQuestDocuments-2024-02-01.2.xls")
diss_meta.df <- bind_rows(abs1.df, abs2.df)
rm("abs1.df","abs2.df")
diss_controlled.df <- diss_meta.df %>% 
  select(isbn,Authors,classification,subjectTerms,pubdate,
         Abstract,Title,identifierKeywords,subjects) %>%
  mutate(classification = replace_na(classification,""),
         subjectTerms = replace_na(subjectTerms,""),
         pubyear_clean = year(as_date(pubdate,format="%Y"))
         ) %>%
  rowwise() %>%
  mutate(classification_clean = 
          str_split_1(classification,pattern = ",") %>% 
           str_squish() %>%
          str_replace("^[0-9]+ ","") %>%
          str_to_lower() %>% unique() %>% list() ,
         subject_terms_clean = str_split_1(subjectTerms,pattern = ",") %>%
           str_squish() %>% str_to_lower() %>% unique() %>%
           list()
  ) %>% 
    ungroup() %>%
    select(isbn, Authors, classification_clean, subject_terms_clean, pubyear_clean,
           Abstract,Title,identifierKeywords,subjects)

rm(diss_meta.df)
```

```{r tokenize}
mystop<-c("i.e","ie","e.g","eg","u.", as.character(1:100),
      "a", "about", "above", "after", "again",  
"am", "an", "and", "any", "are", "aren't", "as", "at", "be", 
"because", "been", "before", "being", "below", "between", "both", 
"but", "by", 
"do", "does", "doing", "don't", "during", 
"each",  "for", "from", "had", "hadn't", "has", 
"hasn't", "have", "haven't", "having", "he", "he'd", "he'll", 
"he's", "her", "here", "here's", "hers", "herself", "him", "himself", 
"his", "how", "how's", "i", "i'd", "i'll", "i'm", "i've", "if", 
"in", "into", "is", "isn't", "it", "it's", "its", "itself", "let's", "no", "nor", 
"not", "of", "off", "on", "or", "ought", 
"our", "ours", "ourselves", "out", "over", "own", "same", "shan't", 
"so",  "such", "than", "that", "that's", "the", "this", "those", "through", 
"to", "too", "under", "until", "up", "very", "was", "wasn't",  "where", "where's", "which")

other_stop <- get_stopwords()[["word"]]

combined_tidy.df <-  diss_controlled.df %>%
  select(isbn,Abstract,Title,identifierKeywords,subjects) %>%
  unite(col="Clean_combined", sep=" ", remove=TRUE, na.rm=TRUE,
        Abstract,Title,identifierKeywords,subjects) %>%
  mutate(Clean_combined=str_replace_all(Clean_combined, "U\\.S\\.","USA")) %>%
  mutate(Clean_combined=str_replace_all(Clean_combined, "[-\\)\\(\\&\"/]"," ")) %>%
  mutate(Clean_combined=str_replace_all(Clean_combined, "[^a-zA-Z \\.]", ""))  %>% 
  mutate(Clean_combined=str_squish(Clean_combined)) %>% 
  unnest_tokens(ngram, "Clean_combined", token = "skip_ngrams", n = 2,k=1,
                stopwords=mystop)  %>%
  #mutate(ngram = textstem::stem_strings(ngram, language="english")) %>%
  mutate(ngram = str_squish(ngram)) %>% 
  filter(!str_detect(ngram,"\\.")) %>%
  filter(ngram!="")

rm(mystop,other_stop)
```

```{r denoise}


stop_ngrams <- c( 
"associated with", 
"interviews with",  
"these findings", 
"with their", 
"my dissertation",
"first chapter",
"second chapter", 
"empirical chapter", 
"my research", 
"depth with", 
"my findings",
"dissertation three", 
"et al",
"third chapter", 
"three studies",
"their with", 
"with social",  
"with other", 
"interactions with",
"taken together", 
"empirical chapters",
"my study", 
"they also", 
"women who",
"who were", 
"findings study", 
"people who", 
"relationships with", 
"study examines", 
"these results", 
"their their", 
"these studies", 
"they were", 
"attitudes toward", 
"since s", 
"these suggest", 
"three papers", 
"women with", 
"along with", 
"these two",
"with more", 
"dissertation examine", 
"students their",  
"their identities", 
"chapter explores", 
"their health", 
"their parents", 
"were likely", 
"with these", 
"chapter two", 
"chapter uses",
"their counterparts",
"with lower",  
"structured with", 
"students with", 
"study explores", 
"these three", 
"together these", 
"health health", 
"students who",
"th century", 
"their identity",
"third study",
"also find",
"chapter three", 
"across three", 
"all three", 
"chapter examine",
"one another", 
"previous research", 
"second paper", 
"were with", 
"first paper", 
"final chapter", 
"with focus", 
"study also", 
"with mental", 
"with parents", 
"with women", 
"women their", 
"social social", 
"within context", 
"individuals with", 
"other hand", 
"other social",  
"following questions", 
"three questions", 
"individuals who", 
"third paper", 
"three distinct",
"while also", 
"with high", 
"address these", 
"children with", 
"conducted with",
"dissertation social", 
"existing literature", 
"health among", 
"results study", 
"their communities", 
"there little", 
"what call", 
"who their", 
"with family", 
"among women", 
"different social", 
"experiences their", 
"findings dissertation", 
"research also", 
"study examine", 
"their political", 
"these factors", 
"they navigate", 
"with black", 
"with data", 
"with levels", 
"with one", 
"women more",
"engagement with",
"finally chapter", 
"level factors", 
"little known", 
"men who", 
"more their", 
"with racial", 
"adults with",
"data with",
"dissertation two", 
"engage with",
"more specifically",
"study social", 
"with different", 
"with health", 
"chapter four", 
"cope with", 
"different types",
"existing research", 
"one most", 
"parents their",
"social dissertation", 
"their they", 
"these changes", 
"these processes", 
"were conducted", 
"who with", 
"with illness", 
"with people", 
"among older", 
"contributes understanding", 
"current study", 
"interact with", 
"late s",
"overall dissertation", 
"people their", 
"prior research", 
"their family", "their status", "them their", "they with", "vary across", 
"what they", "with who", "across different", 
"across united", "chapter analyzes", "data study", 
"however these", "level data", 
"research question", 
"study uses", "support their", 
"these experiences", "these were", "within their", "work with", 
 "based their",  "chapter investigates", 
 "different groups", "early s", 
  "first examine", 
"individuals their",
"methods approach", 
"months ethnographic", 
"months fieldwork", 
"their experience", "their relationships", 
"third empirical", 
"with political", "with students", "years ethnographic", "across all", 
"across groups", "among adults", "children their", 
"different ways", "dissertation with", "impact their", "research social", 
"shape their", "there research", "these data", "using methods", 
"were associated",  "association with", 
 "even when", "examine social", "few studies", "findings also", 
"first two", 
"relationships their", "sense their", 
"their these", "these can", "they can", 
"were less", 
"with greater", "with increased", "chapter dissertation", 
"conclude with", 
"dissertation research", "dissertation use", "questions what", 
"three different", "three dissertation", 
"two different", "with non",
"within social", "women were",  
 "implications research", "mid s", "positively with", 
 "st century", "study chapter", "study three", 
 "these analyses",  "were used", "what extent", "while research", 
"with implications", "with low", "with self", 
"work their",  "other words",  "research with", 
 "their role", "these also", 
"these dissertation", "two chapters", 
"were significantly", "when their",  "with men", 
"with new", "with respect",  
"due their",  "first dissertation",  
"however there", 
"most important", "my first", "my suggest", "social well", "study two", 
"themselves their", "these groups", "these strategies", "three key", 
 "what can",  
"can help",  "consists three", 
 "dissertation also", "experience their", 
"explore these",
"we find", "were also",  "with discussion", 
"also their",  "combined with", "compared their", 
"contact with", "data depth", "data three", "dissertation study", 
"finally study",  
"findings research",
 "methods used", "more more", "more with", 
"other forms", "overall findings", 
"overall study",  "research focused", 
"results these", "school with",  "study with",  
"these chapters",  "three years", 
 "using case", "work conflict",
 "chapter whether", 
 "correlated with", "data dissertation", 
"data were", "despite their",  "however research", 
"men their", "my results",  "other factors", 
 "present study", "research these", "research what", 
"states with", "structural cultural", "study adult", "their everyday", 
 "these provide", "these spaces", 
"three research", "use their", 
"also more", "also show",  "concludes with", 
"examines social",    
"level characteristics", "much more", "my also", 
"my show", "my work", 
"outcomes with", "taken these", 
"their first", "their interactions", "them with", 
 "these differences", "these issues", "these outcomes", 
"these research", "they did", "they face",
"genderwatch"
)

stop_post_tokens<- c("with", "their", "social", "these", "dissertation", "study", 
"research", "more", "they", "chapter",  
 "also", "three",  "who", 
"first",  "were",  "findings", 
"analysis", "use", "while", "within", "two", "my", "other", 
"using", "across", "among", "can", "level", 
 "based",  
 "find", "well", 
"what",  "time", "second", "when",  "studies",  "one",  
"may", "care", 
"however",
"non", 
"them", 
"all",  "there", "used", 
"third", "including",  "s", 
 "four",  "upon", 
 "n", 
"co", "xa", "five", "re", "al", "et", 
"et al")

stop_post.df <- tibble(ngram=c(stop_post_tokens, stop_ngrams))
rm(stop_post_tokens,stop_ngrams)

```

# Corpus-Level Characteristics

## Word and Phrase Frequencies

```{r corpus-frequencies}
unc_ngram_freq.df <-combined_tidy.df %>% 
  anti_join(stop_post.df,by="ngram") %>%
  count(ngram,sort=TRUE) 
```

```{r uncontrolled-terms-frequencies}

suppressWarnings(
  (
    unc_ngram_freq.df %>%
      ggplot(aes(x = n)) + geom_histogram(bins = 100) + scale_y_log10() +
      labs(title="Distribution of 1 & 2 word terms from abs,title, keywords")
  ) %>% plotly::ggplotly()
)

unc_ngram_freq.df  %>% arrange(desc(n)) %>%
      slice_head(n=200) %>%
      rename(freq=n,word=ngram) %>%
      ggwordcloud::ggwordcloud2(size=.8) +
      labs(title="most popular uncontrolled terms") 
    
unc_ngram_freq.df  %>% 
    arrange(desc(n)) %>%
    slice_head(n = 1000) %>%
    DT::datatable (
      data = .,
      extensions = 'Buttons',
      options = list(dom = 'Bfrltip',
                     buttons = c('csv')),
      caption ="Most frequent 1 and 2 word terms in uncontrolled description"
    )
```

```{}
```

# Controlled Classifications

```{r controlled-meta-clean}

diss_controlled.df %>% 
  rename(term=classification_clean) %>%
  select(isbn, term, pubyear_clean) %>% 
  unnest(cols=c(term))  -> diss_class_tidy.df

diss_controlled.df %>% 
  rename(term=subject_terms_clean) %>%
  select(isbn, term, pubyear_clean) %>% 
  unnest(cols=c(term))  -> diss_subj_tidy.df

ndis <- nrow(diss_controlled.df)
lower_q <- .05 
upper_q <- .95

{diss_subj_tidy.df %>% 
  count(term, sort=TRUE) %>%
  filter(n>= lower_q * ndis,
         n<= upper_q * ndis) %>%
  mutate(p = n/ndis) %>%
  ggplot(aes(x=fct_reorder(term,p),y=p))+
  geom_col() +
  coord_flip() +
  labs(x="assigned subjects")} %>% plotly::ggplotly()

{diss_subj_tidy.df %>% 
  filter(pubyear_clean < 2024) %>%
  count(term, pubyear_clean, sort=TRUE) %>%
  rowwise() %>%
  mutate(p=n/ndis,
         p_year=n/dylook.ls[[as.character(pubyear_clean)]]) %>%
  filter(p_year >=  lower_q ,
         p_year <=  upper_q ) %>%
  ggplot(aes(x=fct_reorder(term,p),y=p_year))+
  geom_col() +
  coord_flip() +
  labs(x="assigned subjects") +
  facet_wrap(vars(pubyear_clean))} %>% plotly::ggplotly() 

rm(dylook.ls, diss_year.df)

```

# Dissertation Characteristics

## Terms

```{r absfreq}
abs_doc_freq.df <-
  abs_tidy.df %>% 
  anti_join(stop_post.df,by="ngram") %>%
  count(ngram,isbn) %>% 
  bind_tf_idf(ngram,isbn,n)

ngram_count.df <- abs_doc_freq.df %>% 
  group_by(ngram) %>%
  summarize(num_dissertation=n())

suppressWarnings(
  (
    ngram_count.df %>%
      ggplot(aes(x = num_dissertation)) + geom_histogram(bins = 100) + scale_y_log10()
  ) %>% plotly::ggplotly()
)

ngram_count.df %>% 
  filter(num_dissertation > dim(abs.df)[[1]] *.05, 
         num_dissertation < dim(abs.df)[[1]] *.5  )  %>%
  mutate(percent_dissertation=round(num_dissertation/dim(abs.df)[[1]],digits=2)) %>%
  select(ngram,percent_dissertation) %>%
  arrange(desc(percent_dissertation)) -> core_ngrams.df

  core_ngrams.df %>% 
   DT::datatable (
      data = .,
      extensions = 'Buttons',
      options = list(dom = 'Bfrltip',
                     buttons = c('csv')),
      caption = "ngrams appearing in at least 5% & less than 50% of dissertations"
    )


abs_doc_freq.df %>% 
  group_by(isbn) %>%
  arrange(desc(tf_idf), .by_group=TRUE) %>% 
  slice_head(n=1) %>%
  ungroup() %>%
  arrange(desc(tf_idf)) %>%
   DT::datatable (
      data = .,
      extensions = 'Buttons',
      options = list(dom = 'Bfrltip',
                     buttons = c('csv')),
      caption = "each dissertation abstract's most distinctive word or phrase"
    )

```

## Topics

```{r LDA}

core_dtm <- 
  left_join( core_ngrams.df %>% select(ngram), abs_doc_freq.df, 
             by=join_by(ngram)) %>%
  select(ngram,isbn,n) %>%
  rename(term=ngram, document=isbn, value=n) %>%
  cast_dtm(term=term,document=document,value=value)

LDA.res <- topicmodels::LDA(x = core_dtm, k =20, method="Gibbs")

LDA.topics <- tidy(LDA.res, matrix = "beta")
LDA_top_terms <- LDA.topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 5) %>% 
  ungroup() %>%
  arrange(topic, -beta)

LDA_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

```{r LDA-v2}

# using textmineR

core_sparse <- 
  left_join( core_ngrams.df %>% select(ngram), abs_doc_freq.df, 
             by=join_by(ngram)) %>%
  select(ngram,isbn,n) %>%
  rename(term=ngram, document=isbn, value=n)%>%
  cast_sparse(row=document,column=term,value=value)

LDA.res.tm <- 
  textmineR::FitLdaModel(dtm = core_sparse, 
                 k = 20,
                 iterations = 200,
                 burnin = 175)

LDA.res <- topicmodels::LDA(x = core_dtm, k =20)

tidy.lda_topic_model <-function(x, matrix="beta") {
  
  if (matrix=="beta") {
    res  <- data.frame(topic = as.integer(stringr::str_replace_all(rownames(x$phi), "t_", "")), 
                        x$phi, 
                        stringsAsFactors = FALSE) %>%
  gather(term, beta, -topic) %>% 
  tibble::as_tibble()
    
  } else {
    res <- data.frame(document = rownames(x$theta),
                         x$theta,
                         stringsAsFactors = FALSE) %>%
  gather(topic, gamma, -document) %>%
  tibble::as_tibble()
  }
  res
}

LDA.topics <- tidy(LDA.res.tm, matrix = "beta")
LDA_top_terms <- LDA.topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 5) %>% 
  ungroup() %>%
  arrange(topic, -beta)

LDA_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

```{r topic-cluster}
# document cluster
LDA.docs <-  tidy(LDA.res.tm, matrix = "gamma")

LDA.docs %<>% left_join(by=c("document"="isbn"), 
                        abs.df %>% select(isbn,Title))

LDA_top_topics <- LDA.docs %>%
  group_by(topic) %>%
  slice_max(gamma, n = 5) %>% 
  ungroup() %>%
  arrange(document, -gamma)

LDA_top_topics %>%
   arrange(topic) %>% 
   select(-document) %>%
   DT::datatable (
      data = .,
      extensions = 'Buttons',
      options = list(dom = 'Bfrltip',
                     buttons = c('csv')),
      caption = "each dissertation abstract's most distinctive word or phrase"
    )
```

# Keyword Assisted

```{r fetch-handcodings}

if (!file.exists("data/coded_abstracts.xlsx") || 
    !file.exists("data/ngram_seeds.xlsx")) {
requireNamespace("googledrive")
folderid <- googledrive::as_id("https://drive.google.com/drive/folders/1eFat4mCoRZX22gPLTj9w5Bn63U3q71yo")

filels <- googledrive::drive_ls(folderid)


googledrive::drive_download(
  file = filels %>% filter(name=="abstracts_hand_coding") %>% pull(`id`),
  path = "data/coded_abstracts",
  type= "xlsx",
  overwrite=TRUE)

googledrive::drive_download(
  file = filels %>% filter(name=="socabs_topic_seeds") %>% pull(`id`),
  path = "data/ngram_seeds",
  type= "xlsx",
  overwrite=TRUE)

 rm(folderid, filels)
}
```

```{r create-dictionary}

coded_abs.df <- readxl::read_excel("data/coded_abstracts.xlsx")

#TODO: stem seeds
#TODO: flag seeds missing in corpus

seeds_from_codes.df <- coded_abs.df %>% 
  select(contains(':')) %>% 
  pivot_longer(cols=everything(), names_to="topic", values_to = "ngram") %>% 
  filter(!is.na(ngram) & ngram != 'x') %>%
  separate_longer_delim(ngram,';') %>%
  mutate(ngram=str_squish(ngram)) %>%
  filter(ngram!='')

seeds_from_ngrams.df <- readxl::read_excel("data/ngram_seeds.xlsx") %>%
  rename(topic=Topic,ngram=Ngram) %>% select(topic,ngram)

seeds_from_ngrams.df %<>% 
  separate_longer_delim(topic,";") %>% 
  filter(!is.na(`topic`) & str_detect(`topic`,":")) %>%
  mutate(topic = str_squish(`topic`))  %>%
  filter(topic!="")

seeds.df <- bind_rows(seeds_from_codes.df, seeds_from_ngrams.df) %>% distinct()
  
seeds.df %>% 
  group_by(`topic`) %>% summarise(ngram_list=list(ngram)) %>% 
  pmap( function(topic,ngram_list) { x<-list(); x[[topic]] <- ngram_list; x} ) %>% 
  list_flatten() -> seeds.ls

core_dfm <- 
  left_join(  
            bind_rows(seeds.df %>% select(ngram),
                    core_ngrams.df %>% select(ngram)) %>% distinct(),
             abs_doc_freq.df, 
             by=join_by(ngram)) %>%
  select(ngram,isbn,n) %>%
  rename(term=ngram, document=isbn, value=n) %>% 
  distinct() %>% 
  na.omit() %>% 
  cast_dfm(document=document,term=term,value=value)

```

```{r seededlda}
requireNamespace("quanteda")
requireNamespace("seededlda")

lda_seed <- seededlda::textmodel_seededlda(core_dfm,
                                dict= quanteda::dictionary(seeds.ls),
                                batch_size = 1,
                                residual = 20,
                                auto_iter = TRUE,
                                verbose = TRUE)

seededlda::terms(lda_seed) %>% as_tibble()


dat.df <- attr(lda_seed$data,which="docvars")

dat.df$topic <- seededlda::topics(lda_seed)

dat.df 
```

```{r keyword-ATM}
requireNamespace("quanteda")
requireNamespace("keyATM")
keyATM_docs <- keyATM::keyATM_read(texts = core_dfm)
summary(keyATM_docs)

key_viz <- keyATM::visualize_keywords(docs = keyATM_docs, keywords = seed.ls)
key_viz

lda_key <- keyATM::keyATM(
  docs              = keyATM_docs,    # text input
  no_keyword_topics = 20,              # number of topics without keywords
  keywords          = seed.ls,       # keywords
  model             = "base",         # select the model
)

keyATM::top_words(lda_key)
keyATM::top_docs(lda_key)
keyATM::plot_modelfit(lda_key)
keyATM::plot_alpha(lda_key)
keyATM::plot_pi(lda_key)
```

# Gender and Race of Author

```{r}
  
```
