---
title: "Sociology Abstracts"
format: html
editor: visual
toc: TRUE
code-tools: TRUE
code-fold: TRUE
embed-resources: TRUE
---

## Setup

Load data from proquest on 2020-2024-02-01 sociology department dissertations in English. Clean and tokenise as 2-skip-1 ngrams

```{r setup, include=FALSE}
set.seed(seed=NULL)
library("tidyverse", quietly=TRUE, warn.conflicts=FALSE)
library("magrittr",include.only="%<>%", quietly=TRUE, warn.conflicts=FALSE)
library("tidytext", quietly=TRUE, warn.conflicts=FALSE)
requireNamespace("textstem")
requireNamespace("plotly")
requireNamespace("DT")
requireNamespace("ggwordcloud")
requireNamespace("reshape2")
requireNamespace("topicmodels")
```

```{r, load-abs}
abs1.df <- readxl::read_excel("data/ProQuestDocuments-2024-02-01.xls")
abs2.df <- readxl::read_excel("data/ProQuestDocuments-2024-02-01.2.xls")
diss_meta.df <- bind_rows(abs1.df, abs2.df)
rm("abs1.df","abs2.df")

# NOTE:
# Columns in source overal with each other -- only selected columns used
#  - all duplicates: classification, subjectClassifications, classiifcationCOdes, majorClassificationsCodes
# - subjectTerms appears to be an automated coarse recoding of the classification
# - appears author assigned, duplicate columns: identifierKeywords, subjects
#   apparently post-processed to add "GenderWatch" and "y" tags 

diss_cleaned.df <- diss_meta.df %>% 
  select(isbn, Authors, classification, subjectTerms, pubdate,
         Abstract, Title, identifierKeywords) %>%
  mutate(classification = replace_na(classification,""),
         subjectTerms = replace_na(subjectTerms,""),
         identifierKeywords = replace_na(identifierKeywords,""),
         pubyear_clean = year(as_date(pubdate,format="%Y"))
         ) %>%
  rowwise() %>%
  mutate(classification_clean = 
          str_split_1(classification ,pattern = ",") %>% 
           str_squish() %>%
          str_replace("^[0-9]+ ","") %>%
          str_to_lower() %>% unique() %>% list() ,
         subject_terms_clean = str_split_1(subjectTerms,pattern = ",") %>%
           str_squish() %>% str_to_lower() %>% unique() %>%
           list(), 
          au_identifier_terms_clean =
           str_split_1(identifierKeywords,pattern = ",") %>%
           str_squish() %>% str_to_lower() %>% unique() %>%
           list()
  ) %>% 
    ungroup() %>%
    select(isbn, Authors, classification_clean, subject_terms_clean, pubyear_clean,
           Abstract,Title,identifierKeywords, au_identifier_terms_clean)

diss_cleaned.df %>%
  count(pubyear_clean) %>%
  rename(year=pubyear_clean)-> dy.df
dy.ls <- dy.df %>% pull(n)
names(dy.ls) <- dy.df %>% pull(year)

rm(diss_meta.df)
```

```{r tokenize}
mystop<-c("i.e","ie","e.g","eg","u.", as.character(1:100),
      "a", "about", "above", "after", "again",  
"am", "an", "and", "any", "are", "aren't", "as", "at", "be", 
"because", "been", "before", "being", "below", "between", "both", 
"but", "by", 
"do", "does", "doing", "don't", "during", 
"each",  "for", "from", "had", "hadn't", "has", 
"hasn't", "have", "haven't", "having", "he", "he'd", "he'll", 
"he's", "her", "here", "here's", "hers", "herself", "him", "himself", 
"his", "how", "how's", "i", "i'd", "i'll", "i'm", "i've", "if", 
"in", "into", "is", "isn't", "it", "it's", "its", "itself", "let's", "no", "nor", 
"not", "of", "off", "on", "or", "ought", 
"our", "ours", "ourselves", "out", "over", "own", "same", "shan't", 
"so",  "such", "than", "that", "that's", "the", "this", "those", "through", 
"to", "too", "under", "until", "up", "very", "was", "wasn't",  "where", "where's", "which")

other_stop <- get_stopwords()[["word"]]

combined_tidy.df <-  diss_cleaned.df %>%
  select(isbn,Abstract,Title,identifierKeywords) %>%
  unite(col="Clean_combined", sep=" ", remove=TRUE, na.rm=TRUE,
        Abstract,Title,identifierKeywords) %>%
  mutate(Clean_combined=str_replace_all(Clean_combined, "U\\.S\\.","USA")) %>%
  mutate(Clean_combined=str_replace_all(Clean_combined, "[-\\)\\(\\&\"/]"," ")) %>%
  mutate(Clean_combined=str_replace_all(Clean_combined, "[^a-zA-Z \\.]", ""))  %>% 
  mutate(Clean_combined=str_squish(Clean_combined)) %>% 
  unnest_tokens(ngram, "Clean_combined", token = "skip_ngrams", n = 2,k=1,
                stopwords=mystop)  %>%
  #mutate(ngram = textstem::stem_strings(ngram, language="english")) %>%
  mutate(ngram = str_squish(ngram)) %>% 
  filter(!str_detect(ngram,"\\.")) %>%
  filter(ngram!="")

combined_tf_idf.df <- 
  combined_tidy.df %>% 
  count(ngram,isbn) %>%
    bind_tf_idf(ngram, isbn, n)

rm(mystop,other_stop)
```

```{r denoise}

stop_ngrams <- c( 
"associated with", 
"interviews with",  
"these findings", 
"with their", 
"my dissertation",
"first chapter",
"second chapter", 
"empirical chapter", 
"my research", 
"depth with", 
"my findings",
"dissertation three", 
"et al",
"third chapter", 
"three studies",
"their with", 
"with social",  
"with other", 
"interactions with",
"taken together", 
"empirical chapters",
"my study", 
"they also", 
"women who",
"who were", 
"findings study", 
"people who", 
"relationships with", 
"study examines", 
"these results", 
"their their", 
"these studies", 
"they were", 
"attitudes toward", 
"since s", 
"these suggest", 
"three papers", 
"women with", 
"along with", 
"these two",
"with more", 
"dissertation examine", 
"students their",  
"their identities", 
"chapter explores", 
"their health", 
"their parents", 
"were likely", 
"with these", 
"chapter two", 
"chapter uses",
"their counterparts",
"with lower",  
"structured with", 
"students with", 
"study explores", 
"these three", 
"together these", 
"health health", 
"students who",
"th century", 
"their identity",
"third study",
"also find",
"chapter three", 
"across three", 
"all three", 
"chapter examine",
"one another", 
"previous research", 
"second paper", 
"were with", 
"first paper", 
"final chapter", 
"with focus", 
"study also", 
"with mental", 
"with parents", 
"with women", 
"women their", 
"social social", 
"within context", 
"individuals with", 
"other hand", 
"other social",  
"following questions", 
"three questions", 
"individuals who", 
"third paper", 
"three distinct",
"while also", 
"with high", 
"address these", 
"children with", 
"conducted with",
"dissertation social", 
"existing literature", 
"health among", 
"results study", 
"their communities", 
"there little", 
"what call", 
"who their", 
"with family", 
"among women", 
"different social", 
"experiences their", 
"findings dissertation", 
"research also", 
"study examine", 
"their political", 
"these factors", 
"they navigate", 
"with black", 
"with data", 
"with levels", 
"with one", 
"women more",
"engagement with",
"finally chapter", 
"level factors", 
"little known", 
"men who", 
"more their", 
"with racial", 
"adults with",
"data with",
"dissertation two", 
"engage with",
"more specifically",
"study social", 
"with different", 
"with health", 
"chapter four", 
"cope with", 
"different types",
"existing research", 
"one most", 
"parents their",
"social dissertation", 
"their they", 
"these changes", 
"these processes", 
"were conducted", 
"who with", 
"with illness", 
"with people", 
"among older", 
"contributes understanding", 
"current study", 
"interact with", 
"late s",
"overall dissertation", 
"people their", 
"prior research", 
"their family", "their status", "them their", "they with", "vary across", 
"what they", "with who", "across different", 
"across united", "chapter analyzes", "data study", 
"however these", "level data", 
"research question", 
"study uses", "support their", 
"these experiences", "these were", "within their", "work with", 
 "based their",  "chapter investigates", 
 "different groups", "early s", 
  "first examine", 
"individuals their",
"methods approach", 
"months ethnographic", 
"months fieldwork", 
"their experience", "their relationships", 
"third empirical", 
"with political", "with students", "years ethnographic", "across all", 
"across groups", "among adults", "children their", 
"different ways", "dissertation with", "impact their", "research social", 
"shape their", "there research", "these data", "using methods", 
"were associated",  "association with", 
 "even when", "examine social", "few studies", "findings also", 
"first two", 
"relationships their", "sense their", 
"their these", "these can", "they can", 
"were less", 
"with greater", "with increased", "chapter dissertation", 
"conclude with", 
"dissertation research", "dissertation use", "questions what", 
"three different", "three dissertation", 
"two different", "with non",
"within social", "women were",  
 "implications research", "mid s", "positively with", 
 "st century", "study chapter", "study three", 
 "these analyses",  "were used", "what extent", "while research", 
"with implications", "with low", "with self", 
"work their",  "other words",  "research with", 
 "their role", "these also", 
"these dissertation", "two chapters", 
"were significantly", "when their",  "with men", 
"with new", "with respect",  
"due their",  "first dissertation",  
"however there", 
"most important", "my first", "my suggest", "social well", "study two", 
"themselves their", "these groups", "these strategies", "three key", 
 "what can",  
"can help",  "consists three", 
 "dissertation also", "experience their", 
"explore these",
"we find", "were also",  "with discussion", 
"also their",  "combined with", "compared their", 
"contact with", "data depth", "data three", "dissertation study", 
"finally study",  
"findings research",
 "methods used", "more more", "more with", 
"other forms", "overall findings", 
"overall study",  "research focused", 
"results these", "school with",  "study with",  
"these chapters",  "three years", 
 "using case", "work conflict",
 "chapter whether", 
 "correlated with", "data dissertation", 
"data were", "despite their",  "however research", 
"men their", "my results",  "other factors", 
 "present study", "research these", "research what", 
"states with", "structural cultural", "study adult", "their everyday", 
 "these provide", "these spaces", 
"three research", "use their", 
"also more", "also show",  "concludes with", 
"examines social",    
"level characteristics", "much more", "my also", 
"my show", "my work", 
"outcomes with", "taken these", 
"their first", "their interactions", "them with", 
 "these differences", "these issues", "these outcomes", 
"these research", "they did", "they face",
"genderwatch"
)

stop_post_tokens<- c("with", "their", "social", "these", "dissertation", "study", 
"research", "more", "they", "chapter",  
 "also", "three",  "who", 
"first",  "were",  "findings", 
"analysis", "use", "while", "within", "two", "my", "other", 
"using", "across", "among", "can", "level", 
 "based",  
 "find", "well", 
"what",  "time", "second", "when",  "studies",  "one",  
"may", "care", 
"however",
"non", 
"them", 
"all",  "there", "used", 
"third", "including",  "s", 
 "four",  "upon", 
 "n", 
"y",
"co", "xa", "five", "re", "al", "et", 
"et al")

stop_post.df <- tibble(ngram=c(stop_post_tokens, stop_ngrams))
rm(stop_post_tokens,stop_ngrams)
```

# Distributions of topics (unclustered)

## Dissertations by Controlled Classifications

```{r controlled-meta-clean}

diss_cleaned.df %>% 
  rename(term=classification_clean) %>%
  select(isbn, term, pubyear_clean) %>% 
  unnest(cols=c(term))  -> diss_class_tidy.df

ndis <- sum(dy.ls)
lower_q <- .025 
upper_q <- .95


{diss_class_tidy.df %>% 
  count(term, sort=TRUE) %>%
  mutate(p = n/sum(dy.ls)) %>%
  filter(p>= lower_q ,
         p <= upper_q ) %>%
  ggplot(aes(x=fct_reorder(term,p),y=p))+
  geom_col() +
  coord_flip() +
  labs(x="%age of dissertation assigned to controlled classifications")} %>% plotly::ggplotly()

{diss_class_tidy.df %>% 
  filter(pubyear_clean < 2024) %>%
  count(term, pubyear_clean, sort=TRUE) %>%
  rowwise() %>%
  mutate(p=n/ndis,
         p_year=n/dy.ls[[as.character(pubyear_clean)]]) %>%
  filter(p_year >=  lower_q ,
         p_year <=  upper_q ) %>%
  ggplot(aes(x=fct_reorder(term,p),y=p_year))+
  geom_col() +
  coord_flip() +
  labs(x="%age dissertations in assigned  classification over time") +
  facet_wrap(vars(pubyear_clean))} %>% plotly::ggplotly() 

rm(diss_class_tidy.df, diss_subj_tidy.df, lower_q, upper_q)
```

## Dissertations by Author-Assigned Topics

```{r au-topics}

stop_topics.df <- tibble(term=c("y","genderwatch"))

diss_cleaned.df %>% 
  rename(term=au_identifier_terms_clean) %>%
  select(isbn, term, pubyear_clean) %>% 
  unnest(cols=c(term))  %>%
  anti_join(stop_topics.df,by="term")-> diss_au_id_tidy.df

ndis <- sum(dy.ls)
lower_q <- .01 
upper_q <- 1


{diss_au_id_tidy.df %>% 
  count(term, sort=TRUE) %>%
  mutate(p = n/sum(dy.ls)) %>%
  filter(p>= lower_q ,
         p <= upper_q ) %>%
  ggplot(aes(x=fct_reorder(term,p),y=p))+
  geom_col() +
  coord_flip() +
  labs(x="%age of dissertation by author-assigned keywords")} %>% plotly::ggplotly()

rm(diss_au_id_tidy.df, stop_topics.df)
```

## Dissertations by Terminology

```{r terms-dis-frequencies}

unc_doc_freq.df <-
  combined_tidy.df %>% 
  anti_join(stop_post.df,by="ngram") %>%
  count(ngram,isbn) %>%
  ungroup() %>%
  rename(term=ngram) %>%
  count(term)

suppressWarnings(
  (
    unc_doc_freq.df %>%
      ggplot(aes(x = n)) + geom_histogram(bins = 100) + scale_y_log10() +
      labs(title="Distribution of dissertations that include a term from abs,title, keywords")
  ) %>% plotly::ggplotly()
)

unc_doc_freq.df  %>% arrange(desc(n)) %>%
      slice_head(n=200) %>%
      rename(freq=n,word=term) %>%
      ggwordcloud::ggwordcloud2(size=.8) +
      labs(title="terms appearing in most dissertations") 

lower_q <- .10 
upper_q <- 1

{unc_doc_freq.df %>% 
  mutate(p = n/sum(dy.ls),
         term=fct_reorder(term,p)) %>%
  filter(n>= lower_q * ndis,
         n<= upper_q * ndis) %>%
  ggplot(aes(x=term,y=p))+
  geom_col() +
  coord_flip() +
  labs(x="%age of dissertation assigned to controlled subjects")} %>% plotly::ggplotly()

    
unc_doc_freq.df  %>% 
    arrange(desc(n)) %>%
    slice_head(n = 500) %>%
    DT::datatable (
      data = .,
      extensions = 'Buttons',
      options = list(dom = 'Bfrltip',
                     buttons = c('csv')),
      caption ="Terms appearing in most dissertations"
    )

rm(unc_doc_freq.df)
```

## Distribution of Terminology Across Corpus

```{r uncontrolled-terms-frequencies}

unc_ngram_freq.df <- 
  combined_tidy.df %>% 
  anti_join(stop_post.df,by="ngram") %>%
  count(ngram,sort=TRUE) 

suppressWarnings(
  (
    unc_ngram_freq.df %>%
      ggplot(aes(x = n)) + geom_histogram(bins = 100) + scale_y_log10() +
      labs(title="Distribution of 1 & 2 word terms from abs,title, keywords")
  ) %>% plotly::ggplotly()
)

unc_ngram_freq.df  %>% arrange(desc(n)) %>%
      slice_head(n=200) %>%
      rename(freq=n,word=ngram) %>%
      ggwordcloud::ggwordcloud2(size=.8) +
      labs(title="most popular uncontrolled terms") 
    
unc_ngram_freq.df  %>% 
    arrange(desc(n)) %>%
    slice_head(n = 500) %>%
    DT::datatable (
      data = .,
      extensions = 'Buttons',
      options = list(dom = 'Bfrltip',
                     buttons = c('csv')),
      caption ="Most frequent 1 and 2 word terms in uncontrolled description"
    )

rm(unc_ngram_freq.df)
```

## Most Distinctive Terms in Each Dissertation

```{r tdf-explore}

nterms <- 5
ndiss <- 100

topterms.df <-
  combined_tf_idf.df %>% 
  group_by(isbn) %>%
  slice_max(order_by=tf_idf, n=nterms) 

topdis.df <-
  topterms.df %>%
  group_by(isbn) %>%
  summarize(mean_tf_idf = mean(tf_idf)) %>%
  ungroup() %>%
  slice_max(order_by=mean_tf_idf, n= ndiss)
  
distinctive_diss.df <-
  left_join(topdis.df %>% select(isbn),
            topterms.df %>% select(isbn, ngram),
            by = "isbn") %>% 
  group_by(isbn) %>%
  summarize(distinct_terms = paste(ngram, sep =" ", collapse=", ")) %>% 
  ungroup() %>%
  left_join(diss_cleaned.df %>% select(Title,isbn), by="isbn")


distinctive_diss.df %>%
  relocate(Title, distinct_terms) %>%
  select(-isbn) %>%
  DT::datatable (
      data = .,
      extensions = 'Buttons',
      options = list(dom = 'Bfrltip',
                     buttons = c('csv')),
      caption ="Most distinctive terms in dissertations with distinctive terms"
    )
 
rm(nterms,ndiss, topterms.df, topdis.df, distinctive_diss.df)
```

# Topic Coding

```{r retrieve-expertcodings}

if (!file.exists("data/coded_abstracts.xlsx") || 
    !file.exists("data/ngram_seeds.xlsx")) {
requireNamespace("googledrive")
folderid <- googledrive::as_id("https://drive.google.com/drive/folders/1eFat4mCoRZX22gPLTj9w5Bn63U3q71yo")

filels <- googledrive::drive_ls(folderid)


googledrive::drive_download(
  file = filels %>% filter(name=="abstracts_hand_coding") %>% pull(`id`),
  path = "data/coded_abstracts",
  type= "xlsx",
  overwrite=TRUE)

googledrive::drive_download(
  file = filels %>% filter(name=="socabs_topic_seeds") %>% pull(`id`),
  path = "data/ngram_seeds",
  type= "xlsx",
  overwrite=TRUE)

 rm(folderid, filels)
}
```

```{r create-dictionary}
coded_abs.df <- readxl::read_excel("data/coded_abstracts.xlsx")

#TODO: stem seeds
#TODO: flag seeds missing in corpus

seeds_from_codes.df <- coded_abs.df %>% 
  select(contains(':')) %>% 
  pivot_longer(cols=everything(), names_to="topic", values_to = "ngram") %>% 
  filter(!is.na(ngram) & ngram != 'x') %>%
  separate_longer_delim(ngram,';') %>%
  mutate(ngram=str_squish(ngram)) %>%
  filter(ngram!='')

seeds_from_ngrams.df <- 
  suppressMessages( readxl::read_excel("data/ngram_seeds.xlsx") ) %>%
  rename(topic=Topic,ngram=Ngram) %>% select(topic,ngram)

seeds_from_ngrams.df %<>% 
  separate_longer_delim(topic,";") %>% 
  filter(!is.na(`topic`) & str_detect(`topic`,":")) %>%
  mutate(topic = str_squish(`topic`))  %>%
  filter(topic!="")

seeds.df <- bind_rows(seeds_from_codes.df, seeds_from_ngrams.df) %>% 
  mutate(ngram = str_to_lower(ngram) %>% 
           str_replace_all(pattern="-"," ") %>%
           str_squish()
           ) %>%
  distinct()
  
seeds.df %>% 
  group_by(`topic`) %>% summarise(ngram_list=list(ngram)) %>% 
  pmap( function(topic,ngram_list) { x<-list(); x[[topic]] <- ngram_list; x} ) %>% 
  list_flatten() -> seeds.ls

rm(seeds_from_codes.df, seeds_from_ngrams.df)
```

## Exact Match to Coded Topics Terms

```{r exact-matches}
requireNamespace("gtsummary")
exact_matches.df <- 
  left_join(seeds.df,
            combined_tidy.df %>% distinct(),
            relationship="many-to-many",
            by = "ngram"
            )

nmatched_dis <- exact_matches.df %>% pull(isbn) %>% unique() %>% length

exact_matches.df %>% 
  select(-ngram) %>% 
  distinct() %>% 
  count(topic) %>%
  ungroup() %>%
  separate_wider_delim(topic,names=c("dimension","category"),delim=":") -> topic_sum.df 

topic_sum.df%>%
  ggplot(aes(y=n,x=dimension,fill=category,label=category)) +
  geom_col(position=position_fill()) +
  geom_label(position=position_fill(), vjust=1) +
  theme(legend.position="none") + 
  facet_wrap(vars(dimension), scales="free_x")

topic_sum.df %>% group_by(dimension) %>% gt::gt()


#rm(topic_sum.df)
```

## Topic Models

```{r LDA}

core_dtm <- 
  left_join( core_ngrams.df %>% select(ngram), abs_doc_freq.df, 
             by=join_by(ngram)) %>%
  select(ngram,isbn,n) %>%
  rename(term=ngram, document=isbn, value=n) %>%
  cast_dtm(term=term,document=document,value=value)

LDA.res <- topicmodels::LDA(x = core_dtm, k =20, method="Gibbs")

LDA.topics <- tidy(LDA.res, matrix = "beta")
LDA_top_terms <- LDA.topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 5) %>% 
  ungroup() %>%
  arrange(topic, -beta)

LDA_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

```{r LDA-v2}

# using textmineR

core_sparse <- 
  left_join( core_ngrams.df %>% select(ngram), abs_doc_freq.df, 
             by=join_by(ngram)) %>%
  select(ngram,isbn,n) %>%
  rename(term=ngram, document=isbn, value=n)%>%
  cast_sparse(row=document,column=term,value=value)

LDA.res.tm <- 
  textmineR::FitLdaModel(dtm = core_sparse, 
                 k = 20,
                 iterations = 200,
                 burnin = 175)

LDA.res <- topicmodels::LDA(x = core_dtm, k =20)

tidy.lda_topic_model <-function(x, matrix="beta") {
  
  if (matrix=="beta") {
    res  <- data.frame(topic = as.integer(stringr::str_replace_all(rownames(x$phi), "t_", "")), 
                        x$phi, 
                        stringsAsFactors = FALSE) %>%
  gather(term, beta, -topic) %>% 
  tibble::as_tibble()
    
  } else {
    res <- data.frame(document = rownames(x$theta),
                         x$theta,
                         stringsAsFactors = FALSE) %>%
  gather(topic, gamma, -document) %>%
  tibble::as_tibble()
  }
  res
}

LDA.topics <- tidy(LDA.res.tm, matrix = "beta")
LDA_top_terms <- LDA.topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 5) %>% 
  ungroup() %>%
  arrange(topic, -beta)

LDA_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

```{r topic-cluster}
# document cluster
LDA.docs <-  tidy(LDA.res.tm, matrix = "gamma")

LDA.docs %<>% left_join(by=c("document"="isbn"), 
                        abs.df %>% select(isbn,Title))

LDA_top_topics <- LDA.docs %>%
  group_by(topic) %>%
  slice_max(gamma, n = 5) %>% 
  ungroup() %>%
  arrange(document, -gamma)

LDA_top_topics %>%
   arrange(topic) %>% 
   select(-document) %>%
   DT::datatable (
      data = .,
      extensions = 'Buttons',
      options = list(dom = 'Bfrltip',
                     buttons = c('csv')),
      caption = "each dissertation abstract's most distinctive word or phrase"
    )
```

```{r seeded-dictionary}



core_dfm <- 
  left_join(  
            bind_rows(seeds.df %>% select(ngram),
                    core_ngrams.df %>% select(ngram)) %>% distinct(),
             abs_doc_freq.df, 
             by=join_by(ngram)) %>%
  select(ngram,isbn,n) %>%
  rename(term=ngram, document=isbn, value=n) %>% 
  distinct() %>% 
  na.omit() %>% 
  cast_dfm(document=document,term=term,value=value)

```

```{r seededlda}
requireNamespace("quanteda")
requireNamespace("seededlda")

lda_seed <- seededlda::textmodel_seededlda(core_dfm,
                                dict= quanteda::dictionary(seeds.ls),
                                batch_size = 1,
                                residual = 20,
                                auto_iter = TRUE,
                                verbose = TRUE)

seededlda::terms(lda_seed) %>% as_tibble()


dat.df <- attr(lda_seed$data,which="docvars")

dat.df$topic <- seededlda::topics(lda_seed)

dat.df 
```

```{r keyword-ATM}
requireNamespace("quanteda")
requireNamespace("keyATM")
keyATM_docs <- keyATM::keyATM_read(texts = core_dfm)
summary(keyATM_docs)

key_viz <- keyATM::visualize_keywords(docs = keyATM_docs, keywords = seed.ls)
key_viz

lda_key <- keyATM::keyATM(
  docs              = keyATM_docs,    # text input
  no_keyword_topics = 20,              # number of topics without keywords
  keywords          = seed.ls,       # keywords
  model             = "base",         # select the model
)

keyATM::top_words(lda_key)
keyATM::top_docs(lda_key)
keyATM::plot_modelfit(lda_key)
keyATM::plot_alpha(lda_key)
keyATM::plot_pi(lda_key)
```

# Author Characteristics

```{r}
```
