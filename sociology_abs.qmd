---
title: "Sociology Abstracts"
format: html
editor: visual
toc: TRUE
code-tools: TRUE
code-fold: TRUE
embed-resources: TRUE
---

# Setup

Load data from proquest on 2020-2024-02-01 sociology department dissertations in English. Clean and tokenise as 2-skip-1 ngrams

```{r setup, include=FALSE}
set.seed(seed=NULL)
library("tidyverse", quietly=TRUE, warn.conflicts=FALSE)
library("magrittr",include.only="%<>%", quietly=TRUE, warn.conflicts=FALSE)
library("tidytext", quietly=TRUE, warn.conflicts=FALSE)
requireNamespace("textstem")
requireNamespace("plotly")
requireNamespace("DT")
requireNamespace("ggwordcloud")
requireNamespace("reshape2")
requireNamespace("topicmodels")
```

```{r global-processiong-options}
```

```{r retrieve-expertcodings}
if (!file.exists("data/coded_abstracts.xlsx") || 
    !file.exists("data/ngram_seeds.xlsx") || TRUE
    ) {
requireNamespace("googledrive")
folderid <- googledrive::as_id("https://drive.google.com/drive/folders/1eFat4mCoRZX22gPLTj9w5Bn63U3q71yo")

filels <- googledrive::drive_ls(folderid)

googledrive::drive_download(
  file = filels %>% filter(name=="abstracts_hand_coding") %>% pull(`id`),
  path = "data/coded_abstracts",
  type= "xlsx",
  overwrite=TRUE)

googledrive::drive_download(
  file = filels %>% filter(name=="socabs_topic_seeds") %>% pull(`id`),
  path = "data/ngram_seeds",
  type= "xlsx",
  overwrite=TRUE)

googledrive::drive_download(
  file = filels %>% filter(name=="stems_and_stops") %>% pull(`id`),
  path = "data/stems_and_stops",
  type= "xlsx",
  overwrite=TRUE)

 rm(folderid, filels)
}

coded_abs.df <- readxl::read_excel("data/coded_abstracts.xlsx")
```

```{r, load-abs}
abs1.df <- readxl::read_excel("data/ProQuestDocuments-2024-02-01.xls")
abs2.df <- readxl::read_excel("data/ProQuestDocuments-2024-02-01.2.xls")
diss_meta.df <- bind_rows(abs1.df, abs2.df)
rm("abs1.df","abs2.df")

### text cleaning

clean_text<-function(x) {
  x %>%  
    stringr::str_replace_all("U\\.S\\.","USA") %>% 
    stringr::str_to_lower() %>% 
    stringr::str_replace_all("[-\\)\\(\\&\"/]"," ") %>%
    stringr::str_replace_all("[^a-zA-Z \\.']", "")  %>% 
    stringr::str_squish()
}


# NOTE:
# Columns in source overal with each other -- only selected columns used
#  - all duplicates: classification, subjectClassifications, classiifcationCOdes, majorClassificationsCodes
# - subjectTerms appears to be an automated coarse recoding of the classification
# - appears author assigned, duplicate columns: identifierKeywords, subjects
#   apparently post-processed to add "GenderWatch" and "y" tags 

diss_cleaned.df <- diss_meta.df %>% 
  select(isbn, Authors, classification, subjectTerms, pubdate,
         Abstract, Title, identifierKeywords) %>%
  mutate(classification = replace_na(classification,""),
         subjectTerms = replace_na(subjectTerms,""),
         identifierKeywords = replace_na(identifierKeywords,""),
         pubyear_clean = year(as_date(pubdate,format="%Y"))
         ) %>%
  rowwise() %>%
  mutate(classification_clean = 
          str_split_1(classification ,pattern = ",") %>% 
          str_squish() %>%
          str_replace("^[0-9]+ ","") %>%
          clean_text()  %>%
          unique() %>%
          list() ,
         subject_terms_clean = str_split_1(subjectTerms,pattern = ",") %>%
           clean_text() %>% 
           unique() %>%
           list(), 
         au_identifier_terms_clean =
           str_split_1(identifierKeywords,pattern = ",") %>%
           clean_text() %>% 
           unique() %>%
           list(),
        abstract_clean = 
          clean_text(Abstract),
        title_clean =
          clean_text(Title)
        
  ) %>% 
    ungroup() %>%
    select(isbn, Authors, classification_clean, subject_terms_clean, pubyear_clean,
           Abstract,Title,identifierKeywords, au_identifier_terms_clean, title_clean, abstract_clean)

diss_cleaned.df %>%
  count(pubyear_clean) %>%
  rename(year=pubyear_clean)-> dy.df
dy.ls <- dy.df %>% pull(n)
names(dy.ls) <- dy.df %>% pull(year)

rm(diss_meta.df)
```

# Author Characteristics

```{r gender-coding}
requireNamespace("opengender")
diss_cleaned_plus_gender.df <- diss_cleaned.df %>% 
  mutate(given=  str_split_i(Authors,pattern=",",2)
         %>% str_squish() %>% 
           str_split_i(pattern="[:space:]",1)) 

diss_cleaned_plus_gender.df %<>% opengender::add_gender_predictions(dict= "wgen2")

diss_cleaned_plus_gender.df %>% 
  summarize(opengender::gender_mean(og_details,simplify_output="row"))
```

## Text prep

```{r tokenize}
# subset of the snowball stopwords

minimal_stopwords <-
  c("a",  "am", "an", "and", "any", "are", "aren't", "as", "at", "be","but", "by",  "did", "didn't", 
"do", "does", "doesn't", "doing", "don't", "down", "during", 
"each", "for", "from", "further", "had", "hadn't", "has", 
"hasn't", "have", "haven't", "having", "how", "i", "i'd", "i'll", "i'm", "i've", "if", "in", "into", "is", "isn't", "it", "it's", "its", "itself", "let's", "me", "my", "myself",  "of", "on",  "or", "other",  "so", "than", "that", "that's", "the","their", "they", "them", "then", "there", "there's", "these", "this", "how", "to", "too",  "was", "wasn't", "when", "when's", "where", "where's", "which", "while", "will", "with", "won't")

combined_tidy.df <-  diss_cleaned.df %>%
  select(isbn,abstract_clean,title_clean,au_identifier_terms_clean) %>%
  unite(col="Clean_combined", sep=" ", remove=TRUE, na.rm=TRUE,
        abstract_clean,title_clean,au_identifier_terms_clean) %>%
  unnest_tokens(ngram, "Clean_combined", token = "skip_ngrams", n = 2,k=1,
                stopwords=minimal_stopwords)  %>%
  mutate(ngram = str_squish(ngram)) %>% 
  filter(str_length(ngram)>1)

#%>%
#  filter(!str_detect(ngram,"\\.")) %>%
#  filter(ngram!="")
```

```{r post-stop-and-stem}

if (FALSE) {
  combined_tidy.df %<>%
     mutate(ngram = textstem::stem_strings(ngram, language="english"))
}

stop_post.df <- readxl::read_excel("./data/stems_and_stops.xlsx",
                                       sheet = "stopwords")

stem_raw.df <- readxl::read_excel("./data/stems_and_stops.xlsx",
                                       sheet = "groupwords")

stem_post.df <- stem_raw.df %>%
  dplyr::rowwise() %>%
  mutate( root = stringr::str_split_1(`group`, pattern="; ")[[1]],
          stem = stringr::str_split(`group`, pattern="; ")) %>%
  dplyr::ungroup() %>%
  tidyr::unnest(stem) %>%
  dplyr::filter(root!=stem) %>% 
  select(!group)
  
stem_string <- function(x,delim=" ") {
  longword.df <- stringr::str_split(x,delim)[[1]] %>%
    as_tibble()
  
  longword.df %<>% left_join(stem_post.df, by=c(value="stem"))
  longword.df %<>% mutate(result=coalesce(root,value))
  longword.df %>% pull(result) %>% paste(collapse=delim)
}

stem_reg<-function(x, pattern, delim="; ") {
  pattern_vec=str_split(pattern,delim)[[1]]
  replace_target = paste0(" ",pattern_vec[1]," ")
  replace_pattern = paste0("((^| )",pattern_vec[2:length(pattern_vec)],"($| ))",collapse="|")
  str_replace_all(x,pattern=replace_pattern,replacement=replace_target) %>%
  str_squish()
}

# 10000x faster than join version
stem_string_reg <- function(x) {
  
  reglist <- stem_raw.df %>% pull(group)
  cum <-x
  for (g in reglist) {
     cum <-  stem_reg(cum,g)
  }
  cum
}

#stem_string_reg(c("racial racialize race baiting","foo race", "bar gendered"))

stem_string_V<- Vectorize(stem_string)

combined_tidy_stem.df <- combined_tidy.df %>%
  mutate(ngram=stem_string_reg(ngram)) %>%
  ungroup()

```

```{r stopword}
combined_tf_idf_stemmed.df <- 
  combined_tidy.df %>% 
  count(ngram,isbn) %>%
    bind_tf_idf(ngram, isbn, n)

combined_tf_idf_stemmed.df  %<>%
  anti_join(stop_post.df, by="ngram")
```

# Distributions of topics (unclustered)

## Dissertations by Controlled Classifications

```{r controlled-meta-clean}
diss_cleaned.df %>% 
  rename(term=classification_clean) %>%
  select(isbn, term, pubyear_clean) %>% 
  unnest(cols=c(term)) -> diss_class_tidy.df

ndis <- sum(dy.ls)
lower_q <- .05
upper_q <- .8

{diss_class_tidy.df %>% 
  count(term, sort=TRUE) %>%
  mutate(p = n/sum(dy.ls)) %>%
  filter(p>= lower_q ,
         p <= upper_q ) %>%
  ggplot(aes(x=fct_reorder(term,p),y=p))+
  geom_col() +
  coord_flip() +
  labs(x="%age of dissertation assigned to controlled classifications")} %>% plotly::ggplotly()

{diss_class_tidy.df %>% 
  filter(pubyear_clean < 2024) %>%
  count(term, pubyear_clean, sort=TRUE) %>%
  rowwise() %>%
  mutate(p=n/ndis,
         p_year=n/dy.ls[[as.character(pubyear_clean)]]) %>%
  filter(p_year >=  lower_q ,
         p_year <=  upper_q ) %>%
  ggplot(aes(x=fct_reorder(term,p),y=p_year))+
  geom_col() +
  coord_flip() +
  labs(x="%age dissertations in assigned  classification over time") +
  facet_wrap(vars(pubyear_clean))} %>% plotly::ggplotly() 

rm(diss_class_tidy.df, lower_q, upper_q)
```

## Dissertations by Author-Assigned Topics

```{r au-topics}
stop_topics.df <- tibble(term=c("y","genderwatch"))

diss_cleaned.df %>% 
  rename(term=au_identifier_terms_clean) %>%
  select(isbn, term, pubyear_clean) %>% 
  unnest(cols=c(term))  %>%
  anti_join(stop_topics.df,by="term") %>%
  distinct() -> diss_au_id_tidy.df

lower_q <- .025
upper_q <- 1

{diss_au_id_tidy.df %>% 
  count(term, sort=TRUE) %>%
  mutate(p = n/sum(dy.ls)) %>%
  filter(p>= lower_q ,
         p <= upper_q ) %>%
  ggplot(aes(x=fct_reorder(term,p),y=p))+
  geom_col() +
  coord_flip() +
  labs(x="%age of dissertation by author-assigned keywords")} %>% plotly::ggplotly()

rm(diss_au_id_tidy.df, stop_topics.df)
```

## Dissertations by Terminology

```{r terms-dis-frequencies}
unc_doc_freq.df <-
  combined_tf_idf_stemmed.df %>% 
  count(ngram) %>%
  mutate(p=n/sum(dy.ls))

lower_q <- .25 
upper_q <- .75

unc_doc_freq.df  %>% 
      slice_max(order_by=n, n=200) %>%
      rename(freq=n,word=ngram) %>%
      ggwordcloud::ggwordcloud2(size=.8) +
      labs(title="terms appearing in most dissertations - excluding stopwords") 

{unc_doc_freq.df %>% 
  mutate(p = n/sum(dy.ls),
         ngram=fct_reorder(ngram,p)) %>%
  filter(p>= lower_q,
         p<= upper_q) %>%
  ggplot(aes(x=ngram,y=p))+
  geom_col() +
  coord_flip() +
  labs(x="terms appearing in [25%-75%] of dissertations (excluding stopwords, min 1%)")} %>% plotly::ggplotly()

unc_doc_freq.df  %>% 
    slice_max(order_by=n, n=1000) %>%
    DT::datatable (
      data = .,
      extensions = 'Buttons',
      options = list(dom = 'Bfrltip',
                     buttons = c('csv')),
      caption ="ngrams appearing in most dissertations"
    )

rm(unc_doc_freq.df, lower_q, upper_q)
```

## Distribution of Terminology Across Corpus

```{r uncontrolled-terms-frequencies}
unc_ngram_freq.df <- 
  combined_tf_idf_stemmed.df %>% 
  group_by(ngram) %>%
  summarise(n=sum(n), .groups="drop") %>%
  arrange(n)

unc_ngram_freq.df  %>% 
      slice_max(order_by = n,n=200) %>%
      rename(freq=n,word=ngram) %>%
      ggwordcloud::ggwordcloud2(size=.8) +
      labs(title="most popular uncontrolled terms - excluding stop words") 
    
unc_ngram_freq.df  %>% 
    slice_max(order_by = n,n=1000) %>%
    DT::datatable (
      data = .,
      extensions = 'Buttons',
      options = list(dom = 'Bfrltip',
                     buttons = c('csv')),
      caption ="Most frequent 1 and 2 word terms in uncontrolled description"
    )
rm(unc_ngram_freq.df)
```

## Most Distinctive Terms in Each Dissertation

```{r tdf-explore}
nterms <- 5
ndiss <- 100

topterms.df <-
  combined_tf_idf_stemmed.df %>% 
  group_by(isbn) %>%
  slice_max(order_by=tf_idf, n=nterms) 

topdis.df <-
  topterms.df %>%
  group_by(isbn) %>%
  summarize(mean_tf_idf = mean(tf_idf)) %>%
  ungroup() %>%
  slice_max(order_by=mean_tf_idf, n= ndiss)
  
distinctive_diss.df <-
  left_join(topdis.df %>% select(isbn),
            topterms.df %>% select(isbn, ngram),
            by = "isbn") %>% 
  group_by(isbn) %>%
  summarize(distinct_terms = paste(ngram, sep =" ", collapse=", ")) %>% 
  ungroup() %>%
  left_join(diss_cleaned.df %>% select(Title,isbn), by="isbn")

distinctive_diss.df %>%
  relocate(Title, distinct_terms) %>%
  select(-isbn) %>%
  DT::datatable (
      data = .,
      extensions = 'Buttons',
      options = list(dom = 'Bfrltip',
                     buttons = c('csv')),
      caption ="Most distinctive terms in dissertations with distinctive terms"
    )
 
rm(nterms,ndiss, topterms.df, topdis.df, distinctive_diss.df)
```

# Topic Coding

## Hand Coded Abstracts

```{r coding-setup}
coded_isbns.df <- NULL
```

```{r hand-coded}
hand_codes_tmp.df <-
  coded_abs.df %>% select(isbn,contains(':'),EpiStyle_type) %>% 
  filter( if_any(everything(), ~ !is.na(.x)) )

hand_codes_tmp.df %<>% 
  mutate(EpiStyle_type = str_to_lower(EpiStyle_type)) %>%
  #select(isbn,EpiStyle_type) %>%
  pivot_wider(names_from=EpiStyle_type, values_from=EpiStyle_type,
              names_prefix="epi:") %>% 
  select(-"epi:NA") %>%
  mutate(across(!isbn, 
                  ~ case_when(is.na(.x) ~ FALSE,  .default = TRUE)))

hand_codes_tmp.df %<>%
  pivot_longer(!isbn) %>%
  rename(topic=name) %>%
  mutate(coding_src="hand")

coded_isbns.df %<>% 
  bind_rows(hand_codes_tmp.df)
 
hand_code_tot.df <-
   hand_codes_tmp.df %>% 
    select(-isbn,coding_src) %>%
    filter(value) %>%
    count(topic,value) %>%
    select(-value) %>%
    separate_wider_delim(topic,
                         names=c("dimension","category"),delim=":")

{hand_code_tot.df %>%
    mutate(dimension = as.factor(dimension),
           category=as.factor(category)) %>%
    ggplot(aes(y=n,x=dimension,  fill=category,label=category)) +
    geom_col(position=position_dodge2()) +
    geom_text(position=position_dodge2(width=1), vjust=1) +
    theme(legend.position="none") + 
    facet_wrap(vars(dimension), scales="free_x")} %>%
  plotly::ggplotly()

rm(hand_codes_tmp.df, hand_code_tot.df) 
```

## Exact Match to Coded Topics Terms

```{r prepare-seed-list}
seeds_from_codes.df <- coded_abs.df %>% 
  select(contains(':')) %>% 
  pivot_longer(cols=everything(), names_to="topic", values_to = "ngram") %>% 
  filter(!is.na(ngram) & ngram != 'x') %>%
  separate_longer_delim(ngram,';') %>%
  mutate(ngram=str_squish(ngram)) %>%
  filter(ngram!='')

seeds_from_ngrams.df <- 
  suppressMessages( readxl::read_excel("data/ngram_seeds.xlsx") )%>%
  rename(topic=Topic,ngram=Ngram) %>% select(topic,ngram)

seeds_from_ngrams.df %<>% 
  separate_longer_delim(topic,";") %>% 
  filter(!is.na(`topic`) & str_detect(`topic`,":")) %>%
  mutate(topic = str_squish(`topic`))  %>%
  filter(topic!="")

seeds.df <- bind_rows(seeds_from_codes.df, seeds_from_ngrams.df) %>% 
  mutate(ngram = str_to_lower(ngram) %>% 
           str_replace_all(pattern="-"," ") %>%
           str_squish()
           ) %>%
  distinct()

rm(seeds_from_codes.df, seeds_from_ngrams.df)
```

```{r exact-matches}
exact_matches.df <- 
  left_join(seeds.df,
            combined_tf_idf_stemmed.df %>% distinct(),
            relationship="many-to-many",
            by = "ngram"
            )

exact_matches.df %>% 
  select(topic,ngram,isbn) %>%
  distinct() %>%
  group_by(topic,ngram) %>%
  summarize(n=n(), per_dis=n()/sum(dy.ls), .groups="drop") %>%
  arrange(desc(per_dis)) %>% 
  slice_head(n=15) %>%
  gt::gt() %>%
  gt::tab_header(title="most frequently matched topic terms")
  
nmatched_dis <- exact_matches.df %>% pull(isbn) %>% unique() %>% length

exact_matches.df %<>% 
  select(topic,isbn) %>% 
  distinct() %>%
  mutate(value=TRUE, coding_src="exact")

exact_matches.df %>%
  count(topic) %>%
  ungroup() %>%
  separate_wider_delim(topic,names=c("dimension","category"),delim=":") -> topic_sum.df 

topic_sum.df %<>% 
  group_by(dimension) %>%
  mutate(p=n/sum(dy.ls)) %>%
  ungroup()

lower_q <- .15 
coded_isbns.df %<>% 
  bind_rows(exact_matches.df)

{topic_sum.df%>%
  ggplot(aes(y=p,x=dimension,fill=category,label=category)) +
  geom_col(position=position_dodge2(width=1)) +
  geom_text(position=position_dodge2(width=1), vjust=1) +
  theme(legend.position="none") + 
  facet_wrap(vars(dimension), scales="free_x") +
  labs(x="percent of dissertations that contain topic-seed terms")
  } %>% plotly::ggplotly()

{topic_sum.df%>%
  filter(p>lower_q) %>%
  ggplot(aes(y=p,x=dimension,fill=category,label=category)) +
  geom_col(position=position_dodge2(width=1)) +
  geom_text(position=position_dodge2(width=1), vjust=1) +
  theme(legend.position="none") + 
  facet_wrap(vars(dimension), scales="free_x") +
  labs(x="percent of dissertations that contain topic-seed terms, excluding rare topics")
  } %>% plotly::ggplotly()

topic_sum.df %>% 
  group_by(dimension) %>%
  gt::gt() %>%
  gt::fmt_percent(columns="p")

rm(lower_q,exact_matches.df)
```

## Data Source Matches

Determine geographical scope by implication from referenced data source.

```{r data_matches}
dataset_codes.df <- readxl::read_excel("./data/stems_and_stops.xlsx",
                                       sheet = "datasets") %>%
      mutate(phrase_clean=clean_text(phrase))

dataset_codes_ls.df <-
  dataset_codes.df %>%
  group_by(topic) %>%
  summarise(phrase_list=list(phrase_clean), .groups="drop") %>%
  rowwise() %>% 
 # mutate(reg=paste0("((^| )",phrase_list,"( |$))",collapse="|")) %>% 
 # much slower -- in this case, we can assume that dataset names don't come
 # at very beginning of end
  mutate(reg=paste0("( ",phrase_list," )",collapse="|")) %>% 
  ungroup()

data_matches.df<- purrr::pmap(dataset_codes_ls.df,
    function(topic,reg,...) {
              diss_cleaned_plus_gender.df %>% 
                filter(stringr::str_detect(abstract_clean, reg)) %>% 
                select(isbn) %>% 
                mutate(topic=topic)
    } 
) %>% purrr::list_rbind() %>%
  distinct() %>% 
  mutate(coding_src="data")

coded_isbns.df %<>% 
  bind_rows(data_matches.df)

data_matches.df %>% 
  count(topic) %>%
  gt::gt()
```

## Topic Models

```{r lda-helper}

# tidy helper for keyATM
tidy.keyATM_output<-function(x, matrix="phi", long=FALSE) {
  if (matrix=="phi") {
    res  <- x[["phi"]] %>%
        tibble::as_tibble(rownames=NA) %>%
        tibble::rownames_to_column("topic") 
    if(long) {
      res <-  res %>% 
        tidyr::pivot_longer(!topic) %>%
        dplyr::mutate(term=paste("phi",topic,name,sep="+"),param="phi" ) %>%
        dplyr::relocate(term,estimate=value,topic,word=name)
    }
  } else {
    res <- lda_key[["theta"]]  %>%
        tibble::as_tibble(rownames=NA) %>%
        tibble::rownames_to_column("document")
        if(long) {
    res <-  res %>% tidyr::pivot_longer(!document) %>%
        dplyr::mutate(term=paste("theta",document,name,sep="+"),param="theta" ) %>%
        dplyr::relocate(term,estimate=value,document,topic=name)
    }
  }
  res
}
```

```{r keyword-assisted-topics}
requireNamespace("quanteda")
requireNamespace("keyATM")

topic_lower_q <- .15
unseeded_topics <- 3
seed_prior_strength <- .95
n_subj_categories <- 12
lower_q <-  topic_lower_q
upper_q <- .9
exclude_dim <- c("claim")

#stemming and grouping

combined_tf_idf_stemmed.df %>% 
  group_by(ngram) %>%
  summarize(n=n()) %>%
  mutate(p=n/sum(dy.ls)) -> ngram_diss_ct.df

core_ngrams.df  <-
  ngram_diss_ct.df %>%
  filter( p >= lower_q, p<= upper_q) %>%
  select("ngram") %>%
  bind_rows(seeds.df %>% select(ngram))  %>%
  distinct() %>%
  left_join(combined_tf_idf_stemmed.df %>% select(ngram,isbn,n)) %>%
  na.omit() %>%
  distinct()

core_dfm <- 
  core_ngrams.df %>%
  rename(term=ngram, document=isbn, value=n) %>% 
  cast_dfm(document=document,term=term,value=value)

keyATM_docs <- keyATM::keyATM_read(texts = core_dfm, keep_docnames = TRUE)

index2isbn.df <- tibble(index=keyATM_docs[["doc_index"]], isbn=keyATM_docs[["docnames"]])

index2isbn.df %<>% left_join(diss_cleaned_plus_gender.df %>% select(isbn,Title), by="isbn")

excluded_topics<- topic_sum.df %>%
  filter(p<topic_lower_q) %>% 
  select(dimension,category) %>% 
  transmute(topic=paste(dimension,category,sep=":"))

excluded_areas <- topic_sum.df %>%
    filter(dimension %in% exclude_dim) %>%
      transmute(topic=paste(dimension,category,sep=":"))

excluded_subjects <- topic_sum.df %>%
 filter(dimension=="subj") %>% 
 arrange(desc(p)) %>% 
 slice_tail(n=-1*n_subj_categories ) %>%
 select(dimension,category) %>% 
 transmute(topic=paste(dimension,category,sep=":"))

seeded_topics.ls <- seeds.df %>% 
  anti_join(bind_rows(excluded_topics,excluded_subjects, excluded_areas), by ="topic") %>%
  group_by(`topic`) %>% 
  summarise(ngram_list=list(ngram)) %>% 
  pmap( function(topic,ngram_list) { x<-list(); x[[topic]] <- ngram_list; x} ) %>% 
  list_flatten() 

key_viz <- keyATM::visualize_keywords(docs = keyATM_docs, 
                                      keywords = seeded_topics.ls)
key_viz

suppressMessages(
  lda_key <- keyATM::keyATM(
  docs              = keyATM_docs,    # text input
  no_keyword_topics = unseeded_topics,              # number of topics without keywords
  keywords          = seeded_topics.ls,       # keywords
  priors            = list(beta_s=seed_prior_strength),
  model             = "base",         # select the model
))

lda_key %>% keyATM::plot_modelfit()
keyATM::semantic_coherence(lda_key, keyATM_docs, n = 10)  -> lda_key.sem
lda_key.sem %>% as_tibble(rownames="topic") %>% rename(coherence=value) %>% gt::gt()
```

```{r key-atm-results}
lda_key %>% keyATM::top_words() %>%
  gt::gt() %>% 
  gt::tab_header("Top words for each topic")
  
lda_key %>% keyATM::top_docs(n=5)%>% 
  pivot_longer(cols=everything()) %>%
  rowwise() %>%
  left_join(index2isbn.df, by=c(value="index")) %>%
  ungroup() %>% 
  select(-value,isbn) %>%
  group_by(name) %>%
  gt::gt() %>%
  gt::tab_header("Top dissertations in each topic")

{lda_key %>% keyATM::plot_topicprop(
  n = 5,
  show_topic = NULL,
  show_topwords = TRUE,
  label_topic = NULL,
  order = "proportion")}[["figure"]] %>%
  plotly::ggplotly()
```

```{r document-threhold}

# documents with n% of words on specific topic

tidy(lda_key, matrix="theta", long=TRUE) %>%
  arrange(desc(estimate)) %>% 
  filter(estimate > .2)  %>%
  count(topic) %>%
  gt::gt() %>%
  gt::tab_header("number of dissertations with => 20% words from given topic")

lda_matches.df <- tidy(lda_key, matrix="theta", long=TRUE) %>%
  arrange(desc(estimate)) %>% 
    filter(estimate > .2) %>%
    select(isbn=document, topic=topic) %>%
    mutate(topic=str_replace(topic,"\\d+_","")) %>%
    mutate(topic=str_replace(topic,"^Other","subj:other")) %>%
    distinct() %>%
    mutate(value=TRUE, coding_src="LDA")

coded_isbns.df %<>% filter(coding_src!="LDA")
coded_isbns.df %<>% bind_rows(lda_matches.df)
```

# Bi-variate analysis

```{r coalesce-topic-codings}
coded_isbns_wide.df <- coded_isbns.df %>%
  select(-coding_src) %>%
  filter(value) %>%
  distinct()  %>% 
  pivot_wider(values_from=value, names_from=topic, values_fill=FALSE)

diss_coded_complete.df <-diss_cleaned_plus_gender.df %>% 
  left_join(coded_isbns_wide.df, by="isbn")
```

```{r cumulative-matches}
coded_isbns.df %>%
  filter(value) %>%
  select(-coding_src) %>%
  distinct() %>%
  count(topic,sort=TRUE) %>%
  ungroup() %>%
  separate_wider_delim(topic,names=c("dimension",
                                     "category"),delim=":") ->   all_topic_sum.df 


all_topic_sum.df %<>% 
  group_by(dimension) %>%
  mutate(p=n/sum(dy.ls)) %>%
  ungroup()

all_topic_sum.df


{all_topic_sum.df%>%
  filter(p>lower_q) %>%
  ggplot(aes(y=p,x=dimension,fill=category,label=category)) +
  geom_col(position=position_dodge2(width=1)) +
  geom_text(position=position_dodge2(width=1), vjust=1) +
  theme(legend.position="none") + 
  facet_wrap(vars(dimension), scales="free_x") +
  labs(x="percent of dissertations in each category, excluding rare")
  } %>% plotly::ggplotly()

```

```{r cross-comparison}


coded_isbns_gender.df <- 
  coded_isbns.df %>%
  ungroup() %>%
  filter(value) %>%
  select(!coding_src) %>%
  distinct() %>%
  left_join(diss_cleaned_plus_gender.df %>% select(isbn, og_pr_F), by="isbn") %>%
   separate_wider_delim(topic,names=c("dimension",
                                     "category"),delim=":")


{coded_isbns_gender.df %>%
  group_by(dimension,category) %>%
  summarize(n=n(),p=n/sum(dy.ls), pr_F=mean(og_pr_F,na.rm=TRUE))%>%
  filter(p>lower_q) %>%
  ggplot(aes(y=pr_F,x=dimension,fill=category,label=category)) +
  geom_col(position=position_dodge2(width=1)) +
  geom_text(position=position_dodge2(width=1), vjust=1) +
  theme(legend.position="none") + 
  facet_wrap(vars(dimension), scales="free_x") +
  labs(x="gender ratio of  each category, excluding rare")} %>%
  plotly::ggplotly()
  
# coded_isbns_wide.df %>% xtabs(data =  .,
#       formula = ~ `meth:qual` + `claim:strong`)  %>%
#       ftable() %>% as_tibble() %>% gt::gt()


coded_isbns_wide.df %>% 
  group_by(`meth:qual`) %>%
  summarize(
    across(c(`subj:inequality`,`subj:gender`,`subj:econ`), ~ sum(.x)),
    n=n()
  )

coded_isbns_wide.df %>% 
  group_by(`claim:strong`) %>%
  summarize(
    across(c(`subj:inequality`,`subj:gender`,`subj:econ`), ~ sum(.x)),
    n=n()
  )

coded_isbns_wide.df %>% 
  group_by(`meth:theory`) %>%
  summarize(
    across(c(`subj:inequality`,`subj:gender`,`subj:econ`), ~ sum(.x)),
    n=n()
  )

coded_isbns_wide.df %>% 
  group_by(`epi:pos`) %>%
  summarize(
    across(c(`subj:inequality`,`subj:gender`,`subj:econ`), ~ sum(.x)),
    n=n()
  )

coded_isbns_wide.df %>% 
  group_by(`epi:activist`) %>%
  summarize(
    across(c(`subj:inequality`,`subj:gender`,`subj:econ`), ~ sum(.x)),
    n=n()
  )

coded_isbns_wide.df %>% 
  group_by(`epi:util`) %>%
  summarize(
    across(c(`subj:inequality`,`subj:gender`,`subj:econ`), ~ sum(.x)),
    n=n()
  )

```

```{r}

library(ggalluvial)

coded_isbns_wide.df %>%
  count(`subj:econ`,`meth:qual`,`claim:strong`) %>%
   mutate(`subj:econ`=as.factor(`subj:econ`) 
          %>% fct_recode( econ="TRUE",other_subj="FALSE"),
          `meth:qual`=as.factor(`meth:qual`)  %>%
            fct_recode( qualitative="TRUE",other_meth="FALSE"),
         `claim`=as.factor(`claim:strong`)  %>%
           fct_recode( strong="TRUE",not_strong="FALSE")
          ) %>%
  as.data.frame() %>%
  ggplot(aes(
    y=n,
    axis1=`subj:econ`,
    axis2=`meth:qual`,
  )) +
  geom_alluvium(aes(fill = claim)) +
  geom_stratum() +
  geom_text(stat = "stratum", aes(label = after_stat(stratum))) +
  scale_x_discrete(limits = c("econ", "qualitative"))


vcd::mosaic( ~ `subj:econ` + `meth:qual` +`claim:strong`, data = coded_isbns_wide.df, shade = FALSE)

```
